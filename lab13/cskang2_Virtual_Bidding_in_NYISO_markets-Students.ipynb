{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Virtual bidding in NYISO's markets\n",
    "\n",
    "\n",
    "In this lab, we will implement a simple virtual trading strategy in New York\n",
    "ISO's electricity markets. The goal is to maximize profits. We shall train our\n",
    "model on price data from one year, and implement the strategy on the data from\n",
    "the next year. How much can you earn with a certain daily budget? Say \\$250K?\n",
    "\n",
    "We will present a trading strategy. You are welcome to try other strategies and compare the gains over multiple runs.\n",
    "\n",
    "Let's start with customary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import choice\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the day-ahead and real-time prices from 11 zones in New York.\n",
    "\n",
    "The day-ahead prices are defined every hour. The real-time market runs every 5 minutes. For each zone, an average of\n",
    "these prices over an hour is published.\n",
    "\n",
    "Store the list of zones in the variable 'listOfZones'. Also, store the number of options\n",
    "as the number of zones times the 24 hours available for trading. Finally create another\n",
    "list containing the option names (zone + hour)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "listOfZones = ['CAPITL', 'CENTRL', 'DUNWOD', 'GENESE', 'HUD VL', 'LONGIL', 'MHK VL',\n",
    "               'MILLWD', 'N.Y.C.', 'NORTH', 'WEST']\n",
    "\n",
    "nOptions = len(listOfZones) * 24\n",
    "optionNames = [zone + \"_Hour_\" + str(t) for zone in listOfZones for t in range(24)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse the files with DA and RT prices along with DA load forecast.\n",
    "\n",
    "Define a function that parses three files containing DA and RT prices, along with DA load predictions from a year from all different zones in the list defined before. This function will be used for to both load the data for training the classifiers and testing them. This function has 3 outputs: they are DA prices, difference between DA and RT prices, and finally DA load predictions. The outputs are pandas data frames whose columns are the options, and rows are the days in the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadNYISOData(year):\n",
    "\n",
    "    # Open the relevant files for DA prices, RT prices, DA load.\n",
    "\n",
    "    dfPriceDA = pd.read_csv(\"DAM_NYISO_Zonal_LBMP_\" + str(year) + \".csv\")\n",
    "    dfPriceRT = pd.read_csv(\"RTM_NYISO_Zonal_LBMP_\" + str(year) + \".csv\")\n",
    "    dfLoadDA = pd.read_csv(\"DAM_NYISO_LoadForecast_\" + str(year) + \".csv\")\n",
    "\n",
    "    # Collect the DA and RT prices from each zone from each hour and create a pandas list.\n",
    "    # The data should have prices and loads from all days of a year, where each day\n",
    "    # contributes 24 rows, corresponding to each hour.\n",
    "\n",
    "    priceDA = pd.DataFrame({zone: (dfPriceDA.loc[dfPriceDA['Zone Name'] == zone,\n",
    "                                                 'DAM Zonal LBMP']).values\n",
    "                             for zone in listOfZones})\n",
    "    priceRT = pd.DataFrame({zone: (dfPriceRT.loc[dfPriceRT['Zone Name'] == zone,\n",
    "                                                 'TWI Zonal LBMP']).values\n",
    "                             for zone in listOfZones})\n",
    "    loadDA = pd.DataFrame({zone: (dfLoadDA.loc[dfLoadDA['Zone Name'] == zone,\n",
    "                                               'DAM Forecast Load']).values\n",
    "                            for zone in listOfZones})\n",
    "\n",
    "    numberOfDays = int(len(priceDA.index)/24)\n",
    "\n",
    "    # Compute the price differences between DA and RT prices for all options on\n",
    "    # all days of the year. Store it as a pandas data frame where the 24 rows for\n",
    "    # each day is flattened into one row. This operation essentially allows us to\n",
    "    # independently think of each zone in each hour as a separate option. Also,\n",
    "    # reshape the prices for the DA market in the same manner.\n",
    "\n",
    "    priceDART = pd.DataFrame([priceRT.sub(priceDA).loc[day * 24:\n",
    "                                                              (day + 1) * 24 - 1,\n",
    "                              listOfZones].values.flatten()\n",
    "                              for day in range(numberOfDays)],\n",
    "                             columns=optionNames)\n",
    "\n",
    "    priceDA = pd.DataFrame([priceDA.loc[day * 24: (day + 1) * 24 - 1,\n",
    "                            listOfZones].values.flatten()\n",
    "                            for day in range(numberOfDays)],\n",
    "                           columns=optionNames)\n",
    "\n",
    "    return priceDA, priceDART, loadDA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a function that creates the inputs for training a classifier\n",
    "\n",
    "Create a function that takes the price and load data and creates two arrays\n",
    "'X' and 'Y'. Essentially, the rows of 'X' contains all information relevant to\n",
    " predicting the sign of the price difference on the various options on the next day.\n",
    " It takes as an input, three pandas frames corresponding to the DA prices, price\n",
    " differences, and the DA load predictions, and produces three outputs:\n",
    " the arrays 'X', 'Y', and the range of days from the year that were used to\n",
    " create the data 'X' and 'Y'. This function will be used to both train and\n",
    " test classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createClassifierIO(priceDA, priceDART, loadDA):\n",
    "\n",
    "    # Define how many past days of prices to use for classification.\n",
    "\n",
    "    pastPrices = range(1, 3)\n",
    "\n",
    "    # Define how many past days of load predictions to use for classification.\n",
    "\n",
    "    pastLoad = range(1, 3)\n",
    "\n",
    "    # Define a date range within the year to create the arrays 'X' and 'Y' in a way\n",
    "    # that past price and load data for the first day is within the date range in the\n",
    "    # pandas frames passed as inputs.\n",
    "\n",
    "    rangeOfDays = range(3, len(priceDA.index))\n",
    "\n",
    "    # 'X' will contain three sets of variables:\n",
    "    #   1. the DA prices from past days in the list 'pastDays',\n",
    "    #   2. the differences between DA and RT prices from the same past days,\n",
    "    #   3. the load predictions from past days in the list 'pastLoad'\n",
    "\n",
    "    X = [np.concatenate((\n",
    "        priceDA.loc[[(day - h) for h in pastPrices]].values.flatten(),\n",
    "        priceDART.loc[[(day - h) for h in pastPrices]].values.flatten(),\n",
    "        loadDA.loc[[(day - h) for h in pastLoad]].values.flatten()\n",
    "    )) for day in rangeOfDays]\n",
    "\n",
    "    # Scale the array 'X' to make its data zero mean and unit variance.\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "\n",
    "    # 'Y' will contain zeros and ones, where a one indicates that the price in DA is\n",
    "    # higher than in RT for a particular option. Recall that an option corresponds to\n",
    "    # a zone at a particular hour of the day.\n",
    "\n",
    "    Y = np.array([(priceDART.loc[day].values > 0).astype(int)\n",
    "                  for day in rangeOfDays])\n",
    "\n",
    "    # Return the arrays 'X' and 'Y', and finally the range of days from the year that\n",
    "    # will be utilized for training or testing the classifier.\n",
    "    return X, Y, rangeOfDays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design the training module.\n",
    " The training module utilizes a year's worth of data to determine the following for\n",
    " each option, i.e., for each zone for each hour of the day:\n",
    "   1. Classifiers that predict the sign of the difference between DA and RT prices.\n",
    "   2. Statistics of the mean of the price difference.\n",
    "   3. A quantile of the day-ahead prices that we will use as our bid for each option.\n",
    " You will either train the classifiers here or load them from the folder './Classifiers'.\n",
    " Storing the classifiers from time to time allows you to only vary the bidding strategy\n",
    " and observe the annual reward rather than having to train the classifiers every time.\n",
    " \n",
    "### Define and train the classifiers or load pre-trained classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training module...\n",
      "\n",
      "Loaded hourly prices from 2015 for 365 days.\n",
      "Classifier trained for option CAPITL_Hour_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option CAPITL_Hour_1\n",
      "Classifier trained for option CAPITL_Hour_2\n",
      "Classifier trained for option CAPITL_Hour_3\n",
      "Classifier trained for option CAPITL_Hour_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option CAPITL_Hour_5\n",
      "Classifier trained for option CAPITL_Hour_6\n",
      "Classifier trained for option CAPITL_Hour_7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option CAPITL_Hour_8\n",
      "Classifier trained for option CAPITL_Hour_9\n",
      "Classifier trained for option CAPITL_Hour_10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option CAPITL_Hour_11\n",
      "Classifier trained for option CAPITL_Hour_12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option CAPITL_Hour_13\n",
      "Classifier trained for option CAPITL_Hour_14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option CAPITL_Hour_15\n",
      "Classifier trained for option CAPITL_Hour_16\n",
      "Classifier trained for option CAPITL_Hour_17\n",
      "Classifier trained for option CAPITL_Hour_18\n",
      "Classifier trained for option CAPITL_Hour_19\n",
      "Classifier trained for option CAPITL_Hour_20\n",
      "Classifier trained for option CAPITL_Hour_21\n",
      "Classifier trained for option CAPITL_Hour_22\n",
      "Classifier trained for option CAPITL_Hour_23\n",
      "Classifier trained for option CENTRL_Hour_0\n",
      "Classifier trained for option CENTRL_Hour_1\n",
      "Classifier trained for option CENTRL_Hour_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option CENTRL_Hour_3\n",
      "Classifier trained for option CENTRL_Hour_4\n",
      "Classifier trained for option CENTRL_Hour_5\n",
      "Classifier trained for option CENTRL_Hour_6\n",
      "Classifier trained for option CENTRL_Hour_7\n",
      "Classifier trained for option CENTRL_Hour_8\n",
      "Classifier trained for option CENTRL_Hour_9\n",
      "Classifier trained for option CENTRL_Hour_10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option CENTRL_Hour_11\n",
      "Classifier trained for option CENTRL_Hour_12\n",
      "Classifier trained for option CENTRL_Hour_13\n",
      "Classifier trained for option CENTRL_Hour_14\n",
      "Classifier trained for option CENTRL_Hour_15\n",
      "Classifier trained for option CENTRL_Hour_16\n",
      "Classifier trained for option CENTRL_Hour_17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option CENTRL_Hour_18\n",
      "Classifier trained for option CENTRL_Hour_19\n",
      "Classifier trained for option CENTRL_Hour_20\n",
      "Classifier trained for option CENTRL_Hour_21\n",
      "Classifier trained for option CENTRL_Hour_22\n",
      "Classifier trained for option CENTRL_Hour_23\n",
      "Classifier trained for option DUNWOD_Hour_0\n",
      "Classifier trained for option DUNWOD_Hour_1\n",
      "Classifier trained for option DUNWOD_Hour_2\n",
      "Classifier trained for option DUNWOD_Hour_3\n",
      "Classifier trained for option DUNWOD_Hour_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option DUNWOD_Hour_5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option DUNWOD_Hour_6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option DUNWOD_Hour_7\n",
      "Classifier trained for option DUNWOD_Hour_8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option DUNWOD_Hour_9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option DUNWOD_Hour_10\n",
      "Classifier trained for option DUNWOD_Hour_11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option DUNWOD_Hour_12\n",
      "Classifier trained for option DUNWOD_Hour_13\n",
      "Classifier trained for option DUNWOD_Hour_14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option DUNWOD_Hour_15\n",
      "Classifier trained for option DUNWOD_Hour_16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option DUNWOD_Hour_17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option DUNWOD_Hour_18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option DUNWOD_Hour_19\n",
      "Classifier trained for option DUNWOD_Hour_20\n",
      "Classifier trained for option DUNWOD_Hour_21\n",
      "Classifier trained for option DUNWOD_Hour_22\n",
      "Classifier trained for option DUNWOD_Hour_23\n",
      "Classifier trained for option GENESE_Hour_0\n",
      "Classifier trained for option GENESE_Hour_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option GENESE_Hour_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option GENESE_Hour_3\n",
      "Classifier trained for option GENESE_Hour_4\n",
      "Classifier trained for option GENESE_Hour_5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option GENESE_Hour_6\n",
      "Classifier trained for option GENESE_Hour_7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option GENESE_Hour_8\n",
      "Classifier trained for option GENESE_Hour_9\n",
      "Classifier trained for option GENESE_Hour_10\n",
      "Classifier trained for option GENESE_Hour_11\n",
      "Classifier trained for option GENESE_Hour_12\n",
      "Classifier trained for option GENESE_Hour_13\n",
      "Classifier trained for option GENESE_Hour_14\n",
      "Classifier trained for option GENESE_Hour_15\n",
      "Classifier trained for option GENESE_Hour_16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option GENESE_Hour_17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option GENESE_Hour_18\n",
      "Classifier trained for option GENESE_Hour_19\n",
      "Classifier trained for option GENESE_Hour_20\n",
      "Classifier trained for option GENESE_Hour_21\n",
      "Classifier trained for option GENESE_Hour_22\n",
      "Classifier trained for option GENESE_Hour_23\n",
      "Classifier trained for option HUD VL_Hour_0\n",
      "Classifier trained for option HUD VL_Hour_1\n",
      "Classifier trained for option HUD VL_Hour_2\n",
      "Classifier trained for option HUD VL_Hour_3\n",
      "Classifier trained for option HUD VL_Hour_4\n",
      "Classifier trained for option HUD VL_Hour_5\n",
      "Classifier trained for option HUD VL_Hour_6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option HUD VL_Hour_7\n",
      "Classifier trained for option HUD VL_Hour_8\n",
      "Classifier trained for option HUD VL_Hour_9\n",
      "Classifier trained for option HUD VL_Hour_10\n",
      "Classifier trained for option HUD VL_Hour_11\n",
      "Classifier trained for option HUD VL_Hour_12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option HUD VL_Hour_13\n",
      "Classifier trained for option HUD VL_Hour_14\n",
      "Classifier trained for option HUD VL_Hour_15\n",
      "Classifier trained for option HUD VL_Hour_16\n",
      "Classifier trained for option HUD VL_Hour_17\n",
      "Classifier trained for option HUD VL_Hour_18\n",
      "Classifier trained for option HUD VL_Hour_19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option HUD VL_Hour_20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option HUD VL_Hour_21\n",
      "Classifier trained for option HUD VL_Hour_22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option HUD VL_Hour_23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option LONGIL_Hour_0\n",
      "Classifier trained for option LONGIL_Hour_1\n",
      "Classifier trained for option LONGIL_Hour_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option LONGIL_Hour_3\n",
      "Classifier trained for option LONGIL_Hour_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option LONGIL_Hour_5\n",
      "Classifier trained for option LONGIL_Hour_6\n",
      "Classifier trained for option LONGIL_Hour_7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option LONGIL_Hour_8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option LONGIL_Hour_9\n",
      "Classifier trained for option LONGIL_Hour_10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option LONGIL_Hour_11\n",
      "Classifier trained for option LONGIL_Hour_12\n",
      "Classifier trained for option LONGIL_Hour_13\n",
      "Classifier trained for option LONGIL_Hour_14\n",
      "Classifier trained for option LONGIL_Hour_15\n",
      "Classifier trained for option LONGIL_Hour_16\n",
      "Classifier trained for option LONGIL_Hour_17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option LONGIL_Hour_18\n",
      "Classifier trained for option LONGIL_Hour_19\n",
      "Classifier trained for option LONGIL_Hour_20\n",
      "Classifier trained for option LONGIL_Hour_21\n",
      "Classifier trained for option LONGIL_Hour_22\n",
      "Classifier trained for option LONGIL_Hour_23\n",
      "Classifier trained for option MHK VL_Hour_0\n",
      "Classifier trained for option MHK VL_Hour_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option MHK VL_Hour_2\n",
      "Classifier trained for option MHK VL_Hour_3\n",
      "Classifier trained for option MHK VL_Hour_4\n",
      "Classifier trained for option MHK VL_Hour_5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option MHK VL_Hour_6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option MHK VL_Hour_7\n",
      "Classifier trained for option MHK VL_Hour_8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option MHK VL_Hour_9\n",
      "Classifier trained for option MHK VL_Hour_10\n",
      "Classifier trained for option MHK VL_Hour_11\n",
      "Classifier trained for option MHK VL_Hour_12\n",
      "Classifier trained for option MHK VL_Hour_13\n",
      "Classifier trained for option MHK VL_Hour_14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option MHK VL_Hour_15\n",
      "Classifier trained for option MHK VL_Hour_16\n",
      "Classifier trained for option MHK VL_Hour_17\n",
      "Classifier trained for option MHK VL_Hour_18\n",
      "Classifier trained for option MHK VL_Hour_19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option MHK VL_Hour_20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option MHK VL_Hour_21\n",
      "Classifier trained for option MHK VL_Hour_22\n",
      "Classifier trained for option MHK VL_Hour_23\n",
      "Classifier trained for option MILLWD_Hour_0\n",
      "Classifier trained for option MILLWD_Hour_1\n",
      "Classifier trained for option MILLWD_Hour_2\n",
      "Classifier trained for option MILLWD_Hour_3\n",
      "Classifier trained for option MILLWD_Hour_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option MILLWD_Hour_5\n",
      "Classifier trained for option MILLWD_Hour_6\n",
      "Classifier trained for option MILLWD_Hour_7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option MILLWD_Hour_8\n",
      "Classifier trained for option MILLWD_Hour_9\n",
      "Classifier trained for option MILLWD_Hour_10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option MILLWD_Hour_11\n",
      "Classifier trained for option MILLWD_Hour_12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option MILLWD_Hour_13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option MILLWD_Hour_14\n",
      "Classifier trained for option MILLWD_Hour_15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option MILLWD_Hour_16\n",
      "Classifier trained for option MILLWD_Hour_17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option MILLWD_Hour_18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option MILLWD_Hour_19\n",
      "Classifier trained for option MILLWD_Hour_20\n",
      "Classifier trained for option MILLWD_Hour_21\n",
      "Classifier trained for option MILLWD_Hour_22\n",
      "Classifier trained for option MILLWD_Hour_23\n",
      "Classifier trained for option N.Y.C._Hour_0\n",
      "Classifier trained for option N.Y.C._Hour_1\n",
      "Classifier trained for option N.Y.C._Hour_2\n",
      "Classifier trained for option N.Y.C._Hour_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option N.Y.C._Hour_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option N.Y.C._Hour_5\n",
      "Classifier trained for option N.Y.C._Hour_6\n",
      "Classifier trained for option N.Y.C._Hour_7\n",
      "Classifier trained for option N.Y.C._Hour_8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option N.Y.C._Hour_9\n",
      "Classifier trained for option N.Y.C._Hour_10\n",
      "Classifier trained for option N.Y.C._Hour_11\n",
      "Classifier trained for option N.Y.C._Hour_12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option N.Y.C._Hour_13\n",
      "Classifier trained for option N.Y.C._Hour_14\n",
      "Classifier trained for option N.Y.C._Hour_15\n",
      "Classifier trained for option N.Y.C._Hour_16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option N.Y.C._Hour_17\n",
      "Classifier trained for option N.Y.C._Hour_18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option N.Y.C._Hour_19\n",
      "Classifier trained for option N.Y.C._Hour_20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option N.Y.C._Hour_21\n",
      "Classifier trained for option N.Y.C._Hour_22\n",
      "Classifier trained for option N.Y.C._Hour_23\n",
      "Classifier trained for option NORTH_Hour_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option NORTH_Hour_1\n",
      "Classifier trained for option NORTH_Hour_2\n",
      "Classifier trained for option NORTH_Hour_3\n",
      "Classifier trained for option NORTH_Hour_4\n",
      "Classifier trained for option NORTH_Hour_5\n",
      "Classifier trained for option NORTH_Hour_6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option NORTH_Hour_7\n",
      "Classifier trained for option NORTH_Hour_8\n",
      "Classifier trained for option NORTH_Hour_9\n",
      "Classifier trained for option NORTH_Hour_10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option NORTH_Hour_11\n",
      "Classifier trained for option NORTH_Hour_12\n",
      "Classifier trained for option NORTH_Hour_13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option NORTH_Hour_14\n",
      "Classifier trained for option NORTH_Hour_15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option NORTH_Hour_16\n",
      "Classifier trained for option NORTH_Hour_17\n",
      "Classifier trained for option NORTH_Hour_18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option NORTH_Hour_19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option NORTH_Hour_20\n",
      "Classifier trained for option NORTH_Hour_21\n",
      "Classifier trained for option NORTH_Hour_22\n",
      "Classifier trained for option NORTH_Hour_23\n",
      "Classifier trained for option WEST_Hour_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option WEST_Hour_1\n",
      "Classifier trained for option WEST_Hour_2\n",
      "Classifier trained for option WEST_Hour_3\n",
      "Classifier trained for option WEST_Hour_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option WEST_Hour_5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option WEST_Hour_6\n",
      "Classifier trained for option WEST_Hour_7\n",
      "Classifier trained for option WEST_Hour_8\n",
      "Classifier trained for option WEST_Hour_9\n",
      "Classifier trained for option WEST_Hour_10\n",
      "Classifier trained for option WEST_Hour_11\n",
      "Classifier trained for option WEST_Hour_12\n",
      "Classifier trained for option WEST_Hour_13\n",
      "Classifier trained for option WEST_Hour_14\n",
      "Classifier trained for option WEST_Hour_15\n",
      "Classifier trained for option WEST_Hour_16\n",
      "Classifier trained for option WEST_Hour_17\n",
      "Classifier trained for option WEST_Hour_18\n",
      "Classifier trained for option WEST_Hour_19\n",
      "Classifier trained for option WEST_Hour_20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option WEST_Hour_21\n",
      "Classifier trained for option WEST_Hour_22\n",
      "Classifier trained for option WEST_Hour_23\n",
      "\n",
      "Overall training accuracy = 99.95 percent.\n"
     ]
    }
   ],
   "source": [
    "classifiers = []\n",
    "\n",
    "# We have two options here. Use previous training experience, or learn anew.\n",
    "useSavedClassifiers = False\n",
    "\n",
    "if not useSavedClassifiers:\n",
    "\n",
    "    print(\"Starting training module...\\n\")\n",
    "    trainPriceDA, trainPriceDART, trainLoadDA = loadNYISOData(2015)\n",
    "\n",
    "    numberOfDays =  int(len(trainPriceDA.index))\n",
    "    print(\"Loaded hourly prices from 2015 for %d days.\" % numberOfDays)\n",
    "\n",
    "    # We will implement a trading strategy, where we bid a particular quantile of the past\n",
    "    # DA prices for an option. If you do not know what a quantile means, refer to the Wikipedia\n",
    "    # article on it. Essentially, a 95% quantile of the DA prices equals that value for which\n",
    "    # 95% of the DA prices are below it. Store all quantiles starting from 50% in steps of\n",
    "    # 5% in a dictionary. Store them in a pickle file.\n",
    "\n",
    "    quantilesToStore = [0.70, 0.75, 0.80, 0.85, 0.90, 0.95]\n",
    "    offerPrices = trainPriceDA.quantile(q=quantilesToStore).transpose().to_dict()\n",
    "    pickle.dump(offerPrices, open(\"./Training/OfferPrices\", 'wb'))\n",
    "\n",
    "    # Calculate the average price spread for each option over the entire year. This will guide\n",
    "    # us in choosing our portfolio. Store it as a dictionary. Our bid will choose those\n",
    "    # options that our classifier indicates that they will be profitable and historically they\n",
    "    # have higher average price differences, indicating that they have higher rate of return.\n",
    "    # Store them using pickle.\n",
    "\n",
    "    averagePriceSpread = trainPriceDART.mean(axis=0).transpose().to_dict()\n",
    "    pickle.dump(averagePriceSpread, open(\"./Training/AveragePriceSpread\", 'wb'))\n",
    "\n",
    "    # Create the training dataset using the function 'createClassifierIO' on the prices and\n",
    "    # loads, and store them in 'trainX', and 'trainY'.\n",
    "\n",
    "    trainX, trainY, _ = createClassifierIO(trainPriceDA, trainPriceDART, trainLoadDA)\n",
    "\n",
    "    # Define a collection of classifiers, one for each option. You can try different options, such\n",
    "    # as that based on an SVM, logistic regression, multilayer perceptron based, etc. We will\n",
    "    # measure training accuracy to indicate how well the classifier works on the training dataset.\n",
    "    # However, good training accuracy does not always indicate good test performance.\n",
    "    # Avoid over-fitting.\n",
    "\n",
    "\n",
    "    classifiers = [MLPClassifier(hidden_layer_sizes=(20, 10), max_iter=200)\n",
    "                   for _ in range(nOptions)]\n",
    "\n",
    "    trainingAccuracy = 0\n",
    "\n",
    "    for ii in range(nOptions):\n",
    "        classifiers[ii].fit(trainX, trainY[:, ii])\n",
    "        print(\"Classifier trained for option \" + optionNames[ii])\n",
    "        trainingAccuracy += classifiers[ii].score(trainX, trainY[:, ii])\n",
    "\n",
    "        # Store the classifier.\n",
    "        pickle.dump(classifiers[ii], open(\"./Training/Classifier_\" + optionNames[ii], 'wb'))\n",
    "\n",
    "    print(\"\\nOverall training accuracy = %1.2f percent.\" % (100 * trainingAccuracy/nOptions))\n",
    "\n",
    "    del numberOfDays, trainPriceDA, trainLoadDA, trainPriceDART, trainX, trainY\n",
    "else:\n",
    "\n",
    "    # Load the classifiers, the offer prices at various quantiles, and the average price spreads.\n",
    "\n",
    "    print(\"Loading previously trained variables...\\n\")\n",
    "    classifiers = [pickle.load(open(\"./Training/Classifier_\" + optionNames[ii], 'rb'))\n",
    "                   for ii in range(nOptions)]\n",
    "    offerPrices = pickle.load(open(\"./Training/OfferPrices\", 'rb'))\n",
    "    averagePriceSpread = pickle.load(open(\"./Training/AveragePriceSpread\", 'rb'))\n",
    "\n",
    "    print(\"All training variables were loaded successfully...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the classifier's accuracy on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the testing module...\n",
      "\n",
      "Test Accuracy Stats: Min = 49.86%, Avg = 58.98%, Max = 72.45%\n"
     ]
    }
   ],
   "source": [
    "# First, load the test data from NYISO for the year 2016. Again, utilize our function\n",
    "# named 'loadNYISOData'.\n",
    "\n",
    "print(\"Starting the testing module...\\n\")\n",
    "testPriceDA, testPriceDART, testLoadDA = loadNYISOData(2016)\n",
    "\n",
    "# Create the data for the classifier using the function 'createClassifierIO'.\n",
    "testX, testY, rangeOfDays = createClassifierIO(testPriceDA, testPriceDART, testLoadDA)\n",
    "\n",
    "# The next step is not useful for implementing the trading strategy, but quickly check how accurate\n",
    "# your trained classifiers are for the test data. Training accuracy is not always indicative of\n",
    "# test accuracy.\n",
    "\n",
    "testingAccuracy = [classifiers[ii].score(testX, testY[:, ii]) for ii in range(nOptions)]\n",
    "print(\"Test Accuracy Stats: Min = %.2f%%, Avg = %.2f%%, Max = %.2f%%\" %\n",
    "      (100 * np.min(testingAccuracy),\n",
    "       100 * np.mean(testingAccuracy),\n",
    "       100 * np.max(testingAccuracy)))\n",
    "\n",
    "# Utilize the classifiers to predict the sign of DA - RT prices for each day in 'rangeOfDays' for\n",
    "# the test data. Store the result in a pandas data frame with columns as the option names and the\n",
    "# day in year as index.\n",
    "predictedY = pd.DataFrame(np.column_stack([classifiers[ii].predict(testX) for ii in range(nOptions)]),\n",
    "                          columns=optionNames, index=rangeOfDays)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design and implement the trading strategy. \n",
    "\n",
    "We define a fairly simple trading strategy. Define a total budget that you are willing to spend in the DA market. Recall that we only invest in\n",
    "options where we buy at the DA market and sell at the RT market. When your bid for one unit of an option\n",
    "clears in the DA market, you have to pay the DA price for that option. There are two possibilities:\n",
    "  1. Your bid clears: Therefore, your offer price was higher than the DA price.\n",
    "  2. Your bid does not clear: Then, the DA price was higher than your bid.\n",
    "In both these cases, the maximum you have to pay is your bid. Therefore, we will enforce that your bids\n",
    "across all options in a day does not exceed your total budget.\n",
    "Keep track of how rewards grow (or fall) through the year\n",
    "as you utilize your strategy. Also, keep track of how much rewards you get from each option. We shall\n",
    "visualize these results after implementing the trading strategy over the NYISO data for 2016.\n",
    "\n",
    "Choose the bid prices as a suitable quantile of the historical DA prices. The higher the quantile, the\n",
    "better your chances are that your bid will be cleared. However, a higher quantile also indicates that\n",
    "you are budgeting more money for each option, and hence, you will buy fewer options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 3: Reward (in $) = -79,914\n",
      "Day 4: Reward (in $) = 3,047,816\n",
      "Day 5: Reward (in $) = 24,004\n",
      "Day 6: Reward (in $) = -48,232\n",
      "Day 7: Reward (in $) = -4,390\n",
      "Day 8: Reward (in $) = -45,896\n",
      "Day 9: Reward (in $) = -29,683\n",
      "Day 10: Reward (in $) = -115,252\n",
      "Day 11: Reward (in $) = -82,285\n",
      "Day 12: Reward (in $) = -80,712\n",
      "Day 13: Reward (in $) = -70,541\n",
      "Day 14: Reward (in $) = -41,851\n",
      "Day 15: Reward (in $) = -9,903\n",
      "Day 16: Reward (in $) = 38,882\n",
      "Day 17: Reward (in $) = -65,900\n",
      "Day 18: Reward (in $) = -27,079\n",
      "Day 19: Reward (in $) = 5,218\n",
      "Day 20: Reward (in $) = -6,108\n",
      "Day 21: Reward (in $) = 27,212\n",
      "Day 22: Reward (in $) = 86,513\n",
      "Day 23: Reward (in $) = 29,220\n",
      "Day 24: Reward (in $) = -80,990\n",
      "Day 25: Reward (in $) = -21,767\n",
      "Day 26: Reward (in $) = -16,805\n",
      "Day 27: Reward (in $) = -11,849\n",
      "Day 28: Reward (in $) = -21,746\n",
      "Day 29: Reward (in $) = -23,209\n",
      "Day 30: Reward (in $) = -59,282\n",
      "Day 31: Reward (in $) = -26,292\n",
      "Day 32: Reward (in $) = -8,081\n",
      "Day 33: Reward (in $) = -70,054\n",
      "Day 34: Reward (in $) = -9,782\n",
      "Day 35: Reward (in $) = 16,185\n",
      "Day 36: Reward (in $) = -35,600\n",
      "Day 37: Reward (in $) = -40,048\n",
      "Day 38: Reward (in $) = 20,453\n",
      "Day 39: Reward (in $) = 13,136\n",
      "Day 40: Reward (in $) = -33,806\n",
      "Day 41: Reward (in $) = -44,618\n",
      "Day 42: Reward (in $) = -13,652\n",
      "Day 43: Reward (in $) = -51,683\n",
      "Day 44: Reward (in $) = -31,498\n",
      "Day 45: Reward (in $) = 44,739\n",
      "Day 46: Reward (in $) = 77,288\n",
      "Day 47: Reward (in $) = 44,714\n",
      "Day 48: Reward (in $) = -53,755\n",
      "Day 49: Reward (in $) = -60,424\n",
      "Day 50: Reward (in $) = -51,172\n",
      "Day 51: Reward (in $) = 10,604\n",
      "Day 52: Reward (in $) = -18,995\n",
      "Day 53: Reward (in $) = -1,202\n",
      "Day 54: Reward (in $) = -7,293\n",
      "Day 55: Reward (in $) = -5,161\n",
      "Day 56: Reward (in $) = -43,877\n",
      "Day 57: Reward (in $) = -28,315\n",
      "Day 58: Reward (in $) = -18,897\n",
      "Day 59: Reward (in $) = -15,938\n",
      "Day 60: Reward (in $) = -62,087\n",
      "Day 61: Reward (in $) = -154,307\n",
      "Day 62: Reward (in $) = -7,010\n",
      "Day 63: Reward (in $) = -20,269\n",
      "Day 64: Reward (in $) = -2,301\n",
      "Day 65: Reward (in $) = 145,234\n",
      "Day 66: Reward (in $) = 139,782\n",
      "Day 67: Reward (in $) = -14,566\n",
      "Day 68: Reward (in $) = -38,580\n",
      "Day 69: Reward (in $) = -97,433\n",
      "Day 70: Reward (in $) = 79,572\n",
      "Day 71: Reward (in $) = -9,597\n",
      "Day 72: Reward (in $) = -13,539\n",
      "Day 73: Reward (in $) = -117,610\n",
      "Day 74: Reward (in $) = 717,787\n",
      "Day 75: Reward (in $) = -15,967\n",
      "Day 76: Reward (in $) = -93,496\n",
      "Day 77: Reward (in $) = -73,688\n",
      "Day 78: Reward (in $) = -24,629\n",
      "Day 79: Reward (in $) = -32,282\n",
      "Day 80: Reward (in $) = -24,247\n",
      "Day 81: Reward (in $) = -13,575\n",
      "Day 82: Reward (in $) = 12,797\n",
      "Day 83: Reward (in $) = -7,859\n",
      "Day 84: Reward (in $) = -10,605\n",
      "Day 85: Reward (in $) = 25,282\n",
      "Day 86: Reward (in $) = -32,460\n",
      "Day 87: Reward (in $) = 2,985,351\n",
      "Day 88: Reward (in $) = 916,179\n",
      "Day 89: Reward (in $) = -47,195\n",
      "Day 90: Reward (in $) = -14,378\n",
      "Day 91: Reward (in $) = -14,377\n",
      "Day 92: Reward (in $) = -27,588\n",
      "Day 93: Reward (in $) = 2,976\n",
      "Day 94: Reward (in $) = 10,340\n",
      "Day 95: Reward (in $) = -5,469\n",
      "Day 96: Reward (in $) = 70,822\n",
      "Day 97: Reward (in $) = 8,141\n",
      "Day 98: Reward (in $) = -81,018\n",
      "Day 99: Reward (in $) = -31,307\n",
      "Day 100: Reward (in $) = -20,626\n",
      "Day 101: Reward (in $) = -3,101\n",
      "Day 102: Reward (in $) = -322\n",
      "Day 103: Reward (in $) = 199,358\n",
      "Day 104: Reward (in $) = -67,720\n",
      "Day 105: Reward (in $) = -7,369\n",
      "Day 106: Reward (in $) = 14,194\n",
      "Day 107: Reward (in $) = -15,515\n",
      "Day 108: Reward (in $) = 42,103\n",
      "Day 109: Reward (in $) = -23,108\n",
      "Day 110: Reward (in $) = -41,648\n",
      "Day 111: Reward (in $) = -27,095\n",
      "Day 112: Reward (in $) = 75,591\n",
      "Day 113: Reward (in $) = -56,182\n",
      "Day 114: Reward (in $) = -57,746\n",
      "Day 115: Reward (in $) = -122,815\n",
      "Day 116: Reward (in $) = 875\n",
      "Day 117: Reward (in $) = -36,964\n",
      "Day 118: Reward (in $) = -17,300\n",
      "Day 119: Reward (in $) = -3,122\n",
      "Day 120: Reward (in $) = -4,781\n",
      "Day 121: Reward (in $) = 56,689\n",
      "Day 122: Reward (in $) = -41,607\n",
      "Day 123: Reward (in $) = -66,026\n",
      "Day 124: Reward (in $) = -45,409\n",
      "Day 125: Reward (in $) = -31,347\n",
      "Day 126: Reward (in $) = -67,592\n",
      "Day 127: Reward (in $) = -13,671\n",
      "Day 128: Reward (in $) = -55,872\n",
      "Day 129: Reward (in $) = -94,909\n",
      "Day 130: Reward (in $) = -38,059\n",
      "Day 131: Reward (in $) = 23,386\n",
      "Day 132: Reward (in $) = 135,225\n",
      "Day 133: Reward (in $) = -109,086\n",
      "Day 134: Reward (in $) = -49,164\n",
      "Day 135: Reward (in $) = -11,831\n",
      "Day 136: Reward (in $) = -56,269\n",
      "Day 137: Reward (in $) = -47,429\n",
      "Day 138: Reward (in $) = -10,638\n",
      "Day 139: Reward (in $) = -5,675\n",
      "Day 140: Reward (in $) = -43,420\n",
      "Day 141: Reward (in $) = 17,292\n",
      "Day 142: Reward (in $) = 24,065\n",
      "Day 143: Reward (in $) = 3,938\n",
      "Day 144: Reward (in $) = -61,593\n",
      "Day 145: Reward (in $) = -49,025\n",
      "Day 146: Reward (in $) = -15,752\n",
      "Day 147: Reward (in $) = -66,947\n",
      "Day 148: Reward (in $) = 333,100\n",
      "Day 149: Reward (in $) = 66,573\n",
      "Day 150: Reward (in $) = 52,448\n",
      "Day 151: Reward (in $) = 549,864\n",
      "Day 152: Reward (in $) = 366,556\n",
      "Day 153: Reward (in $) = 66,126\n",
      "Day 154: Reward (in $) = 8,189\n",
      "Day 155: Reward (in $) = -42,132\n",
      "Day 156: Reward (in $) = -45,756\n",
      "Day 157: Reward (in $) = -153,209\n",
      "Day 158: Reward (in $) = -71,249\n",
      "Day 159: Reward (in $) = -35,826\n",
      "Day 160: Reward (in $) = -135,153\n",
      "Day 161: Reward (in $) = 2,967\n",
      "Day 162: Reward (in $) = 147,474\n",
      "Day 163: Reward (in $) = -14,604\n",
      "Day 164: Reward (in $) = -45,707\n",
      "Day 165: Reward (in $) = 150,196\n",
      "Day 166: Reward (in $) = 9,080\n",
      "Day 167: Reward (in $) = 27,873\n",
      "Day 168: Reward (in $) = -1,363\n",
      "Day 169: Reward (in $) = 9,869\n",
      "Day 170: Reward (in $) = -37,942\n",
      "Day 171: Reward (in $) = -66,395\n",
      "Day 172: Reward (in $) = -7,534\n",
      "Day 173: Reward (in $) = -1,363\n",
      "Day 174: Reward (in $) = 8,441\n",
      "Day 175: Reward (in $) = 29,757\n",
      "Day 176: Reward (in $) = -18,837\n",
      "Day 177: Reward (in $) = -65,613\n",
      "Day 178: Reward (in $) = -18,666\n",
      "Day 179: Reward (in $) = -32,129\n",
      "Day 180: Reward (in $) = -37,838\n",
      "Day 181: Reward (in $) = -21,445\n",
      "Day 182: Reward (in $) = -70,504\n",
      "Day 183: Reward (in $) = -81,060\n",
      "Day 184: Reward (in $) = -91,470\n",
      "Day 185: Reward (in $) = -45,317\n",
      "Day 186: Reward (in $) = 89,710\n",
      "Day 187: Reward (in $) = 9,006\n",
      "Day 188: Reward (in $) = 3,778\n",
      "Day 189: Reward (in $) = -330\n",
      "Day 190: Reward (in $) = -108,805\n",
      "Day 191: Reward (in $) = -24,856\n",
      "Day 192: Reward (in $) = -31,454\n",
      "Day 193: Reward (in $) = -64,830\n",
      "Day 194: Reward (in $) = -867\n",
      "Day 195: Reward (in $) = 1,540\n",
      "Day 196: Reward (in $) = 6,877\n",
      "Day 197: Reward (in $) = 9,006\n",
      "Day 198: Reward (in $) = 102,893\n",
      "Day 199: Reward (in $) = -1,385\n",
      "Day 200: Reward (in $) = -45,778\n",
      "Day 201: Reward (in $) = -12,193\n",
      "Day 202: Reward (in $) = 2,470\n",
      "Day 203: Reward (in $) = 701\n",
      "Day 204: Reward (in $) = -12,046\n",
      "Day 205: Reward (in $) = -71,855\n",
      "Day 206: Reward (in $) = -2,127\n",
      "Day 207: Reward (in $) = -4,702\n",
      "Day 208: Reward (in $) = 1,637\n",
      "Day 209: Reward (in $) = 7,578\n",
      "Day 210: Reward (in $) = 120,637\n",
      "Day 211: Reward (in $) = 47,516\n",
      "Day 212: Reward (in $) = 80,242\n",
      "Day 213: Reward (in $) = -4,348\n",
      "Day 214: Reward (in $) = -60,865\n",
      "Day 215: Reward (in $) = -20,632\n",
      "Day 216: Reward (in $) = 5,312\n",
      "Day 217: Reward (in $) = 2,621\n",
      "Day 218: Reward (in $) = 28,633\n",
      "Day 219: Reward (in $) = -29,493\n",
      "Day 220: Reward (in $) = -3,430\n",
      "Day 221: Reward (in $) = -17,793\n",
      "Day 222: Reward (in $) = 17,334\n",
      "Day 223: Reward (in $) = 3,228\n",
      "Day 224: Reward (in $) = 571\n",
      "Day 225: Reward (in $) = 767\n",
      "Day 226: Reward (in $) = -48,350\n",
      "Day 227: Reward (in $) = -715\n",
      "Day 228: Reward (in $) = 2,688\n",
      "Day 229: Reward (in $) = -867\n",
      "Day 230: Reward (in $) = -2,153\n",
      "Day 231: Reward (in $) = 2,002\n",
      "Day 232: Reward (in $) = -48,681\n",
      "Day 233: Reward (in $) = -49,752\n",
      "Day 234: Reward (in $) = -104,934\n",
      "Day 235: Reward (in $) = -133,690\n",
      "Day 236: Reward (in $) = -20,473\n",
      "Day 237: Reward (in $) = 46,933\n",
      "Day 238: Reward (in $) = -12,272\n",
      "Day 239: Reward (in $) = -54,746\n",
      "Day 240: Reward (in $) = -4,050\n",
      "Day 241: Reward (in $) = -1,585\n",
      "Day 242: Reward (in $) = -79,885\n",
      "Day 243: Reward (in $) = -5,008\n",
      "Day 244: Reward (in $) = -50,509\n",
      "Day 245: Reward (in $) = -97,030\n",
      "Day 246: Reward (in $) = -53,424\n",
      "Day 247: Reward (in $) = -5,828\n",
      "Day 248: Reward (in $) = -20,650\n",
      "Day 249: Reward (in $) = -21,872\n",
      "Day 250: Reward (in $) = -18,214\n",
      "Day 251: Reward (in $) = -29\n",
      "Day 252: Reward (in $) = 4,891\n",
      "Day 253: Reward (in $) = 97,610\n",
      "Day 254: Reward (in $) = -11,642\n",
      "Day 255: Reward (in $) = -57,267\n",
      "Day 256: Reward (in $) = -5,104\n",
      "Day 257: Reward (in $) = 2,877,844\n",
      "Day 258: Reward (in $) = -12,900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 259: Reward (in $) = -40,183\n",
      "Day 260: Reward (in $) = -25,864\n",
      "Day 261: Reward (in $) = 22,016\n",
      "Day 262: Reward (in $) = 7,910\n",
      "Day 263: Reward (in $) = -25,917\n",
      "Day 264: Reward (in $) = -35,415\n",
      "Day 265: Reward (in $) = -6,316\n",
      "Day 266: Reward (in $) = -69,443\n",
      "Day 267: Reward (in $) = -8,338\n",
      "Day 268: Reward (in $) = -22,863\n",
      "Day 269: Reward (in $) = -83,327\n",
      "Day 270: Reward (in $) = -86,615\n",
      "Day 271: Reward (in $) = -10,904\n",
      "Day 272: Reward (in $) = -32,246\n",
      "Day 273: Reward (in $) = -31,744\n",
      "Day 274: Reward (in $) = 13,106\n",
      "Day 275: Reward (in $) = 23,739\n",
      "Day 276: Reward (in $) = 49,395\n",
      "Day 277: Reward (in $) = 136,096\n",
      "Day 278: Reward (in $) = 183,349\n",
      "Day 279: Reward (in $) = 51,910\n",
      "Day 280: Reward (in $) = 11,881\n",
      "Day 281: Reward (in $) = -9,677\n",
      "Day 282: Reward (in $) = -3,431\n",
      "Day 283: Reward (in $) = -3,043\n",
      "Day 284: Reward (in $) = 19,975\n",
      "Day 285: Reward (in $) = -61,479\n",
      "Day 286: Reward (in $) = -50,873\n",
      "Day 287: Reward (in $) = -41,035\n",
      "Day 288: Reward (in $) = 12,624\n",
      "Day 289: Reward (in $) = 29,821\n",
      "Day 290: Reward (in $) = 5,926\n",
      "Day 291: Reward (in $) = -45,549\n",
      "Day 292: Reward (in $) = -16,390\n",
      "Day 293: Reward (in $) = -9,913\n",
      "Day 294: Reward (in $) = 27,337\n",
      "Day 295: Reward (in $) = -10,355\n",
      "Day 296: Reward (in $) = -2,546\n",
      "Day 297: Reward (in $) = -35,953\n",
      "Day 298: Reward (in $) = 469,570\n",
      "Day 299: Reward (in $) = -6,852\n",
      "Day 300: Reward (in $) = 8,337\n",
      "Day 301: Reward (in $) = -8,690\n",
      "Day 302: Reward (in $) = -5,271\n",
      "Day 303: Reward (in $) = 4,678\n",
      "Day 304: Reward (in $) = -52,718\n",
      "Day 305: Reward (in $) = -16,695\n",
      "Day 306: Reward (in $) = -10,750\n",
      "Day 307: Reward (in $) = 144,192\n",
      "Day 308: Reward (in $) = -49,210\n",
      "Day 309: Reward (in $) = -23,679\n",
      "Day 310: Reward (in $) = -6,776\n",
      "Day 311: Reward (in $) = -31,049\n",
      "Day 312: Reward (in $) = 39,805\n",
      "Day 313: Reward (in $) = -1,776\n",
      "Day 314: Reward (in $) = -37,721\n",
      "Day 315: Reward (in $) = -47,845\n",
      "Day 316: Reward (in $) = -21,035\n",
      "Day 317: Reward (in $) = -30,618\n",
      "Day 318: Reward (in $) = 62,169\n",
      "Day 319: Reward (in $) = 2,932\n",
      "Day 320: Reward (in $) = 20,932\n",
      "Day 321: Reward (in $) = 20,347\n",
      "Day 322: Reward (in $) = 2,368\n",
      "Day 323: Reward (in $) = 6,561\n",
      "Day 324: Reward (in $) = 16,022\n",
      "Day 325: Reward (in $) = -22,152\n",
      "Day 326: Reward (in $) = -12,709\n",
      "Day 327: Reward (in $) = 20,421\n",
      "Day 328: Reward (in $) = 106,987\n",
      "Day 329: Reward (in $) = 34,782\n",
      "Day 330: Reward (in $) = 4,256\n",
      "Day 331: Reward (in $) = -4,693\n",
      "Day 332: Reward (in $) = -47,114\n",
      "Day 333: Reward (in $) = -75,909\n",
      "Day 334: Reward (in $) = -4,337\n",
      "Day 335: Reward (in $) = -17,917\n",
      "Day 336: Reward (in $) = -23,440\n",
      "Day 337: Reward (in $) = 9,758\n",
      "Day 338: Reward (in $) = -9,486\n",
      "Day 339: Reward (in $) = -30,312\n",
      "Day 340: Reward (in $) = -37,239\n",
      "Day 341: Reward (in $) = -46,728\n",
      "Day 342: Reward (in $) = -5,486\n",
      "Day 343: Reward (in $) = -17,735\n",
      "Day 344: Reward (in $) = 73,692\n",
      "Day 345: Reward (in $) = 87,839\n",
      "Day 346: Reward (in $) = -37,764\n",
      "Day 347: Reward (in $) = 20,237\n",
      "Day 348: Reward (in $) = -28,465\n",
      "Day 349: Reward (in $) = 60,307\n",
      "Day 350: Reward (in $) = -3,516\n",
      "Day 351: Reward (in $) = 121,801\n",
      "Day 352: Reward (in $) = 5,599\n",
      "Day 353: Reward (in $) = -10,072\n",
      "Day 354: Reward (in $) = -43,131\n",
      "Day 355: Reward (in $) = 6,807\n",
      "Day 356: Reward (in $) = -12,828\n",
      "Day 357: Reward (in $) = -93,556\n",
      "Day 358: Reward (in $) = -3,773\n",
      "Day 359: Reward (in $) = -10,703\n",
      "Day 360: Reward (in $) = -31,339\n",
      "Day 361: Reward (in $) = -80,702\n",
      "Day 362: Reward (in $) = 20,283\n",
      "Day 363: Reward (in $) = 127,483\n",
      "Day 364: Reward (in $) = 1,263\n",
      "Day 365: Reward (in $) = -27,208\n",
      "Total money earned over the year (in $) = 8,063,347\n"
     ]
    }
   ],
   "source": [
    "dailyBudget = 250000\n",
    "quantileOffer =0.8\n",
    "\n",
    "# Keep track of your rewards in each day, a cumulative reward over the year, and a total reward over the\n",
    "# year. Also, keep track of the total reward from each option. Store them as dictionaries that are\n",
    "# indexed by day of the year.\n",
    "reward = {}\n",
    "cumulativeReward = {}\n",
    "totalReward = 0\n",
    "optionReturn = dict((option, 0) for option in optionNames)\n",
    "\n",
    "# Implement the trading strategy on each day!\n",
    "\n",
    "for day in rangeOfDays:\n",
    "\n",
    "    reward[day] = 0\n",
    "\n",
    "    # Find the options that your classifier says that should be profitable. Store the profitable option\n",
    "    # names in chosenOptions.\n",
    "    chosenOptionNumbers = np.ravel(list(np.nonzero(predictedY.loc[day].values)))\n",
    "    if np.size(chosenOptionNumbers) == 0:\n",
    "        continue\n",
    "    chosenOptions = [optionNames[i] for i in chosenOptionNumbers]\n",
    "\n",
    "    # Design the portfolio based on average price spreads. Our strategy is that as long as you have not\n",
    "    # exceeded your daily budget, pick an option from the list of 'chosenOptions' probabilistically, where\n",
    "    # the probability of choosing it is proportional to exponential(historical rewards of that option). That\n",
    "    # is, a historically profitable option is chosen more often than one that is not. Keep sampling and\n",
    "    # decreasing your budget with each bid.\n",
    "\n",
    "    # Start with an empty portfolio.\n",
    "\n",
    "    portfolio = dict((option, 0) for option in chosenOptions)\n",
    "\n",
    "    # Calculate the probabilities of choosing each option among the list 'chosenOptions'. Recall that\n",
    "    # 'chosenOptions' contains the options that your classifier indicates as being profitable.\n",
    "    priceSpreads = [1.0 * averagePriceSpread[option] for option in chosenOptions]\n",
    "    probabilityOptions = [np.exp(p) for p in priceSpreads]\n",
    "    probabilityOptions /= np.sum(probabilityOptions)\n",
    "\n",
    "    # Start with your daily budget.\n",
    "    budget = dailyBudget\n",
    "\n",
    "    # Sampling among the profitable options and bid based on them.\n",
    "    while budget > np.median([offerPrices[quantileOffer][option] for option in chosenOptions]):\n",
    "\n",
    "        optionToBuy = choice(chosenOptions, p=probabilityOptions)\n",
    "\n",
    "        if budget >= offerPrices[quantileOffer][optionToBuy]:\n",
    "            portfolio[optionToBuy] += 1\n",
    "            budget -= offerPrices[quantileOffer][optionToBuy]\n",
    "\n",
    "    # Compute the reward from the day. Go through each of the options you have decided to buy.\n",
    "    # If the DA price is lower than the bid price, then your bid is cleared. For each option you\n",
    "    # have bought, you get a reward equal to the DA - RT price.\n",
    "\n",
    "    for option in chosenOptions:\n",
    "        if testPriceDA.at[day, option] < offerPrices[quantileOffer][option]:\n",
    "            rewardOptionDay = testPriceDART.at[day, option] * portfolio[option]\n",
    "            optionReturn[option] += rewardOptionDay\n",
    "            reward[day] += rewardOptionDay\n",
    "\n",
    "    totalReward += reward[day]\n",
    "\n",
    "    # Calculate the cumulative reward in millions of dollars.\n",
    "    cumulativeReward[day] = totalReward/1000000\n",
    "\n",
    "    print(\"Day \" + str(day) + \": Reward (in $) = \" + \"{0:,.0f}\".format(reward[day]))\n",
    "\n",
    "print(\"Total money earned over the year (in $) = \" + \"{0:,.0f}\".format(totalReward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task1:  Visualize the rewards (25 points)\n",
    "\n",
    "We would like to plot the cumulative reward over the year 2016. By cumulative reward on a particular date, we mean the total reward from the start of the year till that date.\n",
    "\n",
    "Also, plot a heat map of the returns from each option. \n",
    "\n",
    "#### Fill in the missing lines below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\matplotlib\\figure.py:2359: UserWarning: This figure includes Axes that are not compatible with tight_layout, so results might be incorrect.\n",
      "  warnings.warn(\"This figure includes Axes that are not compatible \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the cumulative reward over the year 2016. Also, plot a heat map of the returns from\n",
    "# each option.\n",
    "\n",
    "fig, axs = plt.subplots(2, 1, tight_layout=True)\n",
    "axs = axs.ravel()\n",
    "\n",
    "\n",
    "## Enter code here where you plot in axs[0].\n",
    "axs[0].plot(cumulativeReward.keys(), cumulativeReward.values() ) \n",
    "axs[1] = sns.heatmap(np.reshape(list(optionReturn.values()), (len(listOfZones), 24)),\n",
    "                     linewidth=0.5,\n",
    "                     cmap=\"YlGnBu\")\n",
    "axs[1].set_yticklabels(listOfZones, rotation=0)\n",
    "axs[1].set_xticklabels(range(24), rotation=90)\n",
    "axs[1].set_title('Returns on different options')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Choosing a classifier (25 points)\n",
    "\n",
    "We used a multilayer perceptron classifier. Your task is to try out SVM and logistic regression classifiers, and explore which one leads to more profits. Use the relevant functions from 'sklearn'.\n",
    "\n",
    "#### Using MLP \n",
    "<br>Test Accuracy Stats: Min = 47.66%, Avg = 58.78%, Max = 69.70%</br>\n",
    "<br>Total money earned over the year (in $) = 8,063,347</br>\n",
    "\n",
    "#### Using SVM (RBF) \n",
    "<br>Test Accuracy Stats: Min = 49.86%, Avg = 65.57%, Max = 76.31%</br> \n",
    "<br>Total money earned over the year (in $) = 1,070,234</br> \n",
    "\n",
    "#### Using Logistic Regression \n",
    "<br>Test Accuracy Stats: Min = 46.56%, Avg = 57.11%, Max = 66.94%</br>\n",
    "<br>Total money earned over the year (in $) = 3,191,907</br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training module...\n",
      "\n",
      "Loaded hourly prices from 2015 for 365 days.\n",
      "Classifier trained for option CAPITL_Hour_0\n",
      "Classifier trained for option CAPITL_Hour_1\n",
      "Classifier trained for option CAPITL_Hour_2\n",
      "Classifier trained for option CAPITL_Hour_3\n",
      "Classifier trained for option CAPITL_Hour_4\n",
      "Classifier trained for option CAPITL_Hour_5\n",
      "Classifier trained for option CAPITL_Hour_6\n",
      "Classifier trained for option CAPITL_Hour_7\n",
      "Classifier trained for option CAPITL_Hour_8\n",
      "Classifier trained for option CAPITL_Hour_9\n",
      "Classifier trained for option CAPITL_Hour_10\n",
      "Classifier trained for option CAPITL_Hour_11\n",
      "Classifier trained for option CAPITL_Hour_12\n",
      "Classifier trained for option CAPITL_Hour_13\n",
      "Classifier trained for option CAPITL_Hour_14\n",
      "Classifier trained for option CAPITL_Hour_15\n",
      "Classifier trained for option CAPITL_Hour_16\n",
      "Classifier trained for option CAPITL_Hour_17\n",
      "Classifier trained for option CAPITL_Hour_18\n",
      "Classifier trained for option CAPITL_Hour_19\n",
      "Classifier trained for option CAPITL_Hour_20\n",
      "Classifier trained for option CAPITL_Hour_21\n",
      "Classifier trained for option CAPITL_Hour_22\n",
      "Classifier trained for option CAPITL_Hour_23\n",
      "Classifier trained for option CENTRL_Hour_0\n",
      "Classifier trained for option CENTRL_Hour_1\n",
      "Classifier trained for option CENTRL_Hour_2\n",
      "Classifier trained for option CENTRL_Hour_3\n",
      "Classifier trained for option CENTRL_Hour_4\n",
      "Classifier trained for option CENTRL_Hour_5\n",
      "Classifier trained for option CENTRL_Hour_6\n",
      "Classifier trained for option CENTRL_Hour_7\n",
      "Classifier trained for option CENTRL_Hour_8\n",
      "Classifier trained for option CENTRL_Hour_9\n",
      "Classifier trained for option CENTRL_Hour_10\n",
      "Classifier trained for option CENTRL_Hour_11\n",
      "Classifier trained for option CENTRL_Hour_12\n",
      "Classifier trained for option CENTRL_Hour_13\n",
      "Classifier trained for option CENTRL_Hour_14\n",
      "Classifier trained for option CENTRL_Hour_15\n",
      "Classifier trained for option CENTRL_Hour_16\n",
      "Classifier trained for option CENTRL_Hour_17\n",
      "Classifier trained for option CENTRL_Hour_18\n",
      "Classifier trained for option CENTRL_Hour_19\n",
      "Classifier trained for option CENTRL_Hour_20\n",
      "Classifier trained for option CENTRL_Hour_21\n",
      "Classifier trained for option CENTRL_Hour_22\n",
      "Classifier trained for option CENTRL_Hour_23\n",
      "Classifier trained for option DUNWOD_Hour_0\n",
      "Classifier trained for option DUNWOD_Hour_1\n",
      "Classifier trained for option DUNWOD_Hour_2\n",
      "Classifier trained for option DUNWOD_Hour_3\n",
      "Classifier trained for option DUNWOD_Hour_4\n",
      "Classifier trained for option DUNWOD_Hour_5\n",
      "Classifier trained for option DUNWOD_Hour_6\n",
      "Classifier trained for option DUNWOD_Hour_7\n",
      "Classifier trained for option DUNWOD_Hour_8\n",
      "Classifier trained for option DUNWOD_Hour_9\n",
      "Classifier trained for option DUNWOD_Hour_10\n",
      "Classifier trained for option DUNWOD_Hour_11\n",
      "Classifier trained for option DUNWOD_Hour_12\n",
      "Classifier trained for option DUNWOD_Hour_13\n",
      "Classifier trained for option DUNWOD_Hour_14\n",
      "Classifier trained for option DUNWOD_Hour_15\n",
      "Classifier trained for option DUNWOD_Hour_16\n",
      "Classifier trained for option DUNWOD_Hour_17\n",
      "Classifier trained for option DUNWOD_Hour_18\n",
      "Classifier trained for option DUNWOD_Hour_19\n",
      "Classifier trained for option DUNWOD_Hour_20\n",
      "Classifier trained for option DUNWOD_Hour_21\n",
      "Classifier trained for option DUNWOD_Hour_22\n",
      "Classifier trained for option DUNWOD_Hour_23\n",
      "Classifier trained for option GENESE_Hour_0\n",
      "Classifier trained for option GENESE_Hour_1\n",
      "Classifier trained for option GENESE_Hour_2\n",
      "Classifier trained for option GENESE_Hour_3\n",
      "Classifier trained for option GENESE_Hour_4\n",
      "Classifier trained for option GENESE_Hour_5\n",
      "Classifier trained for option GENESE_Hour_6\n",
      "Classifier trained for option GENESE_Hour_7\n",
      "Classifier trained for option GENESE_Hour_8\n",
      "Classifier trained for option GENESE_Hour_9\n",
      "Classifier trained for option GENESE_Hour_10\n",
      "Classifier trained for option GENESE_Hour_11\n",
      "Classifier trained for option GENESE_Hour_12\n",
      "Classifier trained for option GENESE_Hour_13\n",
      "Classifier trained for option GENESE_Hour_14\n",
      "Classifier trained for option GENESE_Hour_15\n",
      "Classifier trained for option GENESE_Hour_16\n",
      "Classifier trained for option GENESE_Hour_17\n",
      "Classifier trained for option GENESE_Hour_18\n",
      "Classifier trained for option GENESE_Hour_19\n",
      "Classifier trained for option GENESE_Hour_20\n",
      "Classifier trained for option GENESE_Hour_21\n",
      "Classifier trained for option GENESE_Hour_22\n",
      "Classifier trained for option GENESE_Hour_23\n",
      "Classifier trained for option HUD VL_Hour_0\n",
      "Classifier trained for option HUD VL_Hour_1\n",
      "Classifier trained for option HUD VL_Hour_2\n",
      "Classifier trained for option HUD VL_Hour_3\n",
      "Classifier trained for option HUD VL_Hour_4\n",
      "Classifier trained for option HUD VL_Hour_5\n",
      "Classifier trained for option HUD VL_Hour_6\n",
      "Classifier trained for option HUD VL_Hour_7\n",
      "Classifier trained for option HUD VL_Hour_8\n",
      "Classifier trained for option HUD VL_Hour_9\n",
      "Classifier trained for option HUD VL_Hour_10\n",
      "Classifier trained for option HUD VL_Hour_11\n",
      "Classifier trained for option HUD VL_Hour_12\n",
      "Classifier trained for option HUD VL_Hour_13\n",
      "Classifier trained for option HUD VL_Hour_14\n",
      "Classifier trained for option HUD VL_Hour_15\n",
      "Classifier trained for option HUD VL_Hour_16\n",
      "Classifier trained for option HUD VL_Hour_17\n",
      "Classifier trained for option HUD VL_Hour_18\n",
      "Classifier trained for option HUD VL_Hour_19\n",
      "Classifier trained for option HUD VL_Hour_20\n",
      "Classifier trained for option HUD VL_Hour_21\n",
      "Classifier trained for option HUD VL_Hour_22\n",
      "Classifier trained for option HUD VL_Hour_23\n",
      "Classifier trained for option LONGIL_Hour_0\n",
      "Classifier trained for option LONGIL_Hour_1\n",
      "Classifier trained for option LONGIL_Hour_2\n",
      "Classifier trained for option LONGIL_Hour_3\n",
      "Classifier trained for option LONGIL_Hour_4\n",
      "Classifier trained for option LONGIL_Hour_5\n",
      "Classifier trained for option LONGIL_Hour_6\n",
      "Classifier trained for option LONGIL_Hour_7\n",
      "Classifier trained for option LONGIL_Hour_8\n",
      "Classifier trained for option LONGIL_Hour_9\n",
      "Classifier trained for option LONGIL_Hour_10\n",
      "Classifier trained for option LONGIL_Hour_11\n",
      "Classifier trained for option LONGIL_Hour_12\n",
      "Classifier trained for option LONGIL_Hour_13\n",
      "Classifier trained for option LONGIL_Hour_14\n",
      "Classifier trained for option LONGIL_Hour_15\n",
      "Classifier trained for option LONGIL_Hour_16\n",
      "Classifier trained for option LONGIL_Hour_17\n",
      "Classifier trained for option LONGIL_Hour_18\n",
      "Classifier trained for option LONGIL_Hour_19\n",
      "Classifier trained for option LONGIL_Hour_20\n",
      "Classifier trained for option LONGIL_Hour_21\n",
      "Classifier trained for option LONGIL_Hour_22\n",
      "Classifier trained for option LONGIL_Hour_23\n",
      "Classifier trained for option MHK VL_Hour_0\n",
      "Classifier trained for option MHK VL_Hour_1\n",
      "Classifier trained for option MHK VL_Hour_2\n",
      "Classifier trained for option MHK VL_Hour_3\n",
      "Classifier trained for option MHK VL_Hour_4\n",
      "Classifier trained for option MHK VL_Hour_5\n",
      "Classifier trained for option MHK VL_Hour_6\n",
      "Classifier trained for option MHK VL_Hour_7\n",
      "Classifier trained for option MHK VL_Hour_8\n",
      "Classifier trained for option MHK VL_Hour_9\n",
      "Classifier trained for option MHK VL_Hour_10\n",
      "Classifier trained for option MHK VL_Hour_11\n",
      "Classifier trained for option MHK VL_Hour_12\n",
      "Classifier trained for option MHK VL_Hour_13\n",
      "Classifier trained for option MHK VL_Hour_14\n",
      "Classifier trained for option MHK VL_Hour_15\n",
      "Classifier trained for option MHK VL_Hour_16\n",
      "Classifier trained for option MHK VL_Hour_17\n",
      "Classifier trained for option MHK VL_Hour_18\n",
      "Classifier trained for option MHK VL_Hour_19\n",
      "Classifier trained for option MHK VL_Hour_20\n",
      "Classifier trained for option MHK VL_Hour_21\n",
      "Classifier trained for option MHK VL_Hour_22\n",
      "Classifier trained for option MHK VL_Hour_23\n",
      "Classifier trained for option MILLWD_Hour_0\n",
      "Classifier trained for option MILLWD_Hour_1\n",
      "Classifier trained for option MILLWD_Hour_2\n",
      "Classifier trained for option MILLWD_Hour_3\n",
      "Classifier trained for option MILLWD_Hour_4\n",
      "Classifier trained for option MILLWD_Hour_5\n",
      "Classifier trained for option MILLWD_Hour_6\n",
      "Classifier trained for option MILLWD_Hour_7\n",
      "Classifier trained for option MILLWD_Hour_8\n",
      "Classifier trained for option MILLWD_Hour_9\n",
      "Classifier trained for option MILLWD_Hour_10\n",
      "Classifier trained for option MILLWD_Hour_11\n",
      "Classifier trained for option MILLWD_Hour_12\n",
      "Classifier trained for option MILLWD_Hour_13\n",
      "Classifier trained for option MILLWD_Hour_14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained for option MILLWD_Hour_15\n",
      "Classifier trained for option MILLWD_Hour_16\n",
      "Classifier trained for option MILLWD_Hour_17\n",
      "Classifier trained for option MILLWD_Hour_18\n",
      "Classifier trained for option MILLWD_Hour_19\n",
      "Classifier trained for option MILLWD_Hour_20\n",
      "Classifier trained for option MILLWD_Hour_21\n",
      "Classifier trained for option MILLWD_Hour_22\n",
      "Classifier trained for option MILLWD_Hour_23\n",
      "Classifier trained for option N.Y.C._Hour_0\n",
      "Classifier trained for option N.Y.C._Hour_1\n",
      "Classifier trained for option N.Y.C._Hour_2\n",
      "Classifier trained for option N.Y.C._Hour_3\n",
      "Classifier trained for option N.Y.C._Hour_4\n",
      "Classifier trained for option N.Y.C._Hour_5\n",
      "Classifier trained for option N.Y.C._Hour_6\n",
      "Classifier trained for option N.Y.C._Hour_7\n",
      "Classifier trained for option N.Y.C._Hour_8\n",
      "Classifier trained for option N.Y.C._Hour_9\n",
      "Classifier trained for option N.Y.C._Hour_10\n",
      "Classifier trained for option N.Y.C._Hour_11\n",
      "Classifier trained for option N.Y.C._Hour_12\n",
      "Classifier trained for option N.Y.C._Hour_13\n",
      "Classifier trained for option N.Y.C._Hour_14\n",
      "Classifier trained for option N.Y.C._Hour_15\n",
      "Classifier trained for option N.Y.C._Hour_16\n",
      "Classifier trained for option N.Y.C._Hour_17\n",
      "Classifier trained for option N.Y.C._Hour_18\n",
      "Classifier trained for option N.Y.C._Hour_19\n",
      "Classifier trained for option N.Y.C._Hour_20\n",
      "Classifier trained for option N.Y.C._Hour_21\n",
      "Classifier trained for option N.Y.C._Hour_22\n",
      "Classifier trained for option N.Y.C._Hour_23\n",
      "Classifier trained for option NORTH_Hour_0\n",
      "Classifier trained for option NORTH_Hour_1\n",
      "Classifier trained for option NORTH_Hour_2\n",
      "Classifier trained for option NORTH_Hour_3\n",
      "Classifier trained for option NORTH_Hour_4\n",
      "Classifier trained for option NORTH_Hour_5\n",
      "Classifier trained for option NORTH_Hour_6\n",
      "Classifier trained for option NORTH_Hour_7\n",
      "Classifier trained for option NORTH_Hour_8\n",
      "Classifier trained for option NORTH_Hour_9\n",
      "Classifier trained for option NORTH_Hour_10\n",
      "Classifier trained for option NORTH_Hour_11\n",
      "Classifier trained for option NORTH_Hour_12\n",
      "Classifier trained for option NORTH_Hour_13\n",
      "Classifier trained for option NORTH_Hour_14\n",
      "Classifier trained for option NORTH_Hour_15\n",
      "Classifier trained for option NORTH_Hour_16\n",
      "Classifier trained for option NORTH_Hour_17\n",
      "Classifier trained for option NORTH_Hour_18\n",
      "Classifier trained for option NORTH_Hour_19\n",
      "Classifier trained for option NORTH_Hour_20\n",
      "Classifier trained for option NORTH_Hour_21\n",
      "Classifier trained for option NORTH_Hour_22\n",
      "Classifier trained for option NORTH_Hour_23\n",
      "Classifier trained for option WEST_Hour_0\n",
      "Classifier trained for option WEST_Hour_1\n",
      "Classifier trained for option WEST_Hour_2\n",
      "Classifier trained for option WEST_Hour_3\n",
      "Classifier trained for option WEST_Hour_4\n",
      "Classifier trained for option WEST_Hour_5\n",
      "Classifier trained for option WEST_Hour_6\n",
      "Classifier trained for option WEST_Hour_7\n",
      "Classifier trained for option WEST_Hour_8\n",
      "Classifier trained for option WEST_Hour_9\n",
      "Classifier trained for option WEST_Hour_10\n",
      "Classifier trained for option WEST_Hour_11\n",
      "Classifier trained for option WEST_Hour_12\n",
      "Classifier trained for option WEST_Hour_13\n",
      "Classifier trained for option WEST_Hour_14\n",
      "Classifier trained for option WEST_Hour_15\n",
      "Classifier trained for option WEST_Hour_16\n",
      "Classifier trained for option WEST_Hour_17\n",
      "Classifier trained for option WEST_Hour_18\n",
      "Classifier trained for option WEST_Hour_19\n",
      "Classifier trained for option WEST_Hour_20\n",
      "Classifier trained for option WEST_Hour_21\n",
      "Classifier trained for option WEST_Hour_22\n",
      "Classifier trained for option WEST_Hour_23\n"
     ]
    }
   ],
   "source": [
    "classifiers = []\n",
    "\n",
    "# We have two options here. Use previous training experience, or learn anew.\n",
    "useSavedClassifiers = False\n",
    "\n",
    "if not useSavedClassifiers:\n",
    "\n",
    "    print(\"Starting training module...\\n\")\n",
    "    trainPriceDA, trainPriceDART, trainLoadDA = loadNYISOData(2015)\n",
    "\n",
    "    numberOfDays =  int(len(trainPriceDA.index))\n",
    "    print(\"Loaded hourly prices from 2015 for %d days.\" % numberOfDays)\n",
    "\n",
    "    # We will implement a trading strategy, where we bid a particular quantile of the past\n",
    "    # DA prices for an option. If you do not know what a quantile means, refer to the Wikipedia\n",
    "    # article on it. Essentially, a 95% quantile of the DA prices equals that value for which\n",
    "    # 95% of the DA prices are below it. Store all quantiles starting from 50% in steps of\n",
    "    # 5% in a dictionary. Store them in a pickle file.\n",
    "\n",
    "    quantilesToStore = [0.70, 0.75, 0.80, 0.85, 0.90, 0.95]\n",
    "    offerPrices = trainPriceDA.quantile(q=quantilesToStore).transpose().to_dict()\n",
    "    pickle.dump(offerPrices, open(\"./Training/OfferPrices\", 'wb'))\n",
    "\n",
    "    # Calculate the average price spread for each option over the entire year. This will guide\n",
    "    # us in choosing our portfolio. Store it as a dictionary. Our bid will choose those\n",
    "    # options that our classifier indicates that they will be profitable and historically they\n",
    "    # have higher average price differences, indicating that they have higher rate of return.\n",
    "    # Store them using pickle.\n",
    "\n",
    "    averagePriceSpread = trainPriceDART.mean(axis=0).transpose().to_dict()\n",
    "    pickle.dump(averagePriceSpread, open(\"./Training/AveragePriceSpread\", 'wb'))\n",
    "\n",
    "    # Create the training dataset using the function 'createClassifierIO' on the prices and\n",
    "    # loads, and store them in 'trainX', and 'trainY'.\n",
    "\n",
    "    trainX, trainY, _ = createClassifierIO(trainPriceDA, trainPriceDART, trainLoadDA)\n",
    "\n",
    "    # Define a collection of classifiers, one for each option. You can try different options, such\n",
    "    # as that based on an SVM, logistic regression, multilayer perceptron based, etc. We will\n",
    "    # measure training accuracy to indicate how well the classifier works on the training dataset.\n",
    "    # However, good training accuracy does not always indicate good test performance.\n",
    "    # Avoid over-fitting.\n",
    "\n",
    "    # SVC( degree = 3, kernel = 'rbf' )\n",
    "    #LogisticRegression( penalty = 'l2', solver = 'liblinear' )\n",
    "    \n",
    "    classifiers = [ SVC( degree = 3, kernel = 'rbf', max_iter = 100000 )\n",
    "                   for _ in range(nOptions)]\n",
    "\n",
    "    trainingAccuracy = 0\n",
    "\n",
    "    for ii in range(nOptions):\n",
    "        classifiers[ii].fit(trainX, trainY[:, ii])\n",
    "        print(\"Classifier trained for option \" + optionNames[ii])\n",
    "        trainingAccuracy += classifiers[ii].score(trainX, trainY[:, ii])\n",
    "\n",
    "        # Store the classifier.\n",
    "        pickle.dump(classifiers[ii], open(\"./Training/Classifier_\" + optionNames[ii], 'wb'))\n",
    "\n",
    "    # print(\"\\nOverall training accuracy = %1.2f percent.\" % (100 * trainingAccuracy/nOptions))\n",
    "\n",
    "    del numberOfDays, trainPriceDA, trainLoadDA, trainPriceDART, trainX, trainY\n",
    "else:\n",
    "\n",
    "    # Load the classifiers, the offer prices at various quantiles, and the average price spreads.\n",
    "\n",
    "    # print(\"Loading previously trained variables...\\n\")\n",
    "    classifiers = [pickle.load(open(\"./Training/Classifier_\" + optionNames[ii], 'rb'))\n",
    "                   for ii in range(nOptions)]\n",
    "    offerPrices = pickle.load(open(\"./Training/OfferPrices\", 'rb'))\n",
    "    averagePriceSpread = pickle.load(open(\"./Training/AveragePriceSpread\", 'rb'))\n",
    "\n",
    "    # print(\"All training variables were loaded successfully...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the testing module...\n",
      "\n",
      "Test Accuracy Stats: Min = 49.86%, Avg = 65.57%, Max = 76.31%\n"
     ]
    }
   ],
   "source": [
    "# First, load the test data from NYISO for the year 2016. Again, utilize our function\n",
    "# named 'loadNYISOData'.\n",
    "\n",
    "print(\"Starting the testing module...\\n\")\n",
    "testPriceDA, testPriceDART, testLoadDA = loadNYISOData(2016)\n",
    "\n",
    "# Create the data for the classifier using the function 'createClassifierIO'.\n",
    "testX, testY, rangeOfDays = createClassifierIO(testPriceDA, testPriceDART, testLoadDA)\n",
    "\n",
    "# The next step is not useful for implementing the trading strategy, but quickly check how accurate\n",
    "# your trained classifiers are for the test data. Training accuracy is not always indicative of\n",
    "# test accuracy.\n",
    "\n",
    "testingAccuracy = [classifiers[ii].score(testX, testY[:, ii]) for ii in range(nOptions)]\n",
    "print(\"Test Accuracy Stats: Min = %.2f%%, Avg = %.2f%%, Max = %.2f%%\" %\n",
    "      (100 * np.min(testingAccuracy),\n",
    "       100 * np.mean(testingAccuracy),\n",
    "       100 * np.max(testingAccuracy)))\n",
    "\n",
    "# Utilize the classifiers to predict the sign of DA - RT prices for each day in 'rangeOfDays' for\n",
    "# the test data. Store the result in a pandas data frame with columns as the option names and the\n",
    "# day in year as index.\n",
    "predictedY = pd.DataFrame(np.column_stack([classifiers[ii].predict(testX) for ii in range(nOptions)]),\n",
    "                          columns=optionNames, index=rangeOfDays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 3: Reward (in $) = -67,771\n",
      "Day 4: Reward (in $) = 1,474,679\n",
      "Day 5: Reward (in $) = 24,912\n",
      "Day 6: Reward (in $) = -12,113\n",
      "Day 7: Reward (in $) = -33,755\n",
      "Day 8: Reward (in $) = 1,285\n",
      "Day 9: Reward (in $) = -40,245\n",
      "Day 10: Reward (in $) = -104,691\n",
      "Day 11: Reward (in $) = -66,352\n",
      "Day 12: Reward (in $) = -19,281\n",
      "Day 13: Reward (in $) = -28,826\n",
      "Day 14: Reward (in $) = -37,943\n",
      "Day 15: Reward (in $) = -531\n",
      "Day 16: Reward (in $) = 41,759\n",
      "Day 17: Reward (in $) = -68,051\n",
      "Day 18: Reward (in $) = -22,119\n",
      "Day 19: Reward (in $) = 14,669\n",
      "Day 20: Reward (in $) = -6,074\n",
      "Day 21: Reward (in $) = 30,659\n",
      "Day 22: Reward (in $) = 125,346\n",
      "Day 23: Reward (in $) = 27,844\n",
      "Day 24: Reward (in $) = -86,351\n",
      "Day 25: Reward (in $) = -88,326\n",
      "Day 26: Reward (in $) = -18,248\n",
      "Day 27: Reward (in $) = -7,270\n",
      "Day 28: Reward (in $) = -12,538\n",
      "Day 29: Reward (in $) = -21,999\n",
      "Day 30: Reward (in $) = -34,914\n",
      "Day 31: Reward (in $) = -47,259\n",
      "Day 32: Reward (in $) = -11,417\n",
      "Day 33: Reward (in $) = -57,878\n",
      "Day 34: Reward (in $) = 1,300\n",
      "Day 35: Reward (in $) = 14,209\n",
      "Day 36: Reward (in $) = -40,277\n",
      "Day 37: Reward (in $) = -42,437\n",
      "Day 38: Reward (in $) = 28,247\n",
      "Day 39: Reward (in $) = 20,085\n",
      "Day 40: Reward (in $) = -30,589\n",
      "Day 41: Reward (in $) = 12,184\n",
      "Day 42: Reward (in $) = -15,732\n",
      "Day 43: Reward (in $) = -32,366\n",
      "Day 44: Reward (in $) = -25,179\n",
      "Day 45: Reward (in $) = -17,902\n",
      "Day 46: Reward (in $) = 89,922\n",
      "Day 47: Reward (in $) = 53,741\n",
      "Day 48: Reward (in $) = -56,057\n",
      "Day 49: Reward (in $) = -45,965\n",
      "Day 50: Reward (in $) = -49,225\n",
      "Day 51: Reward (in $) = -1,535\n",
      "Day 52: Reward (in $) = -25,484\n",
      "Day 53: Reward (in $) = -14,465\n",
      "Day 54: Reward (in $) = -1,014\n",
      "Day 55: Reward (in $) = -4,998\n",
      "Day 56: Reward (in $) = -43,604\n",
      "Day 57: Reward (in $) = -28,354\n",
      "Day 58: Reward (in $) = -26,201\n",
      "Day 59: Reward (in $) = -16,004\n",
      "Day 60: Reward (in $) = -7,206\n",
      "Day 61: Reward (in $) = -120,068\n",
      "Day 62: Reward (in $) = -14,211\n",
      "Day 63: Reward (in $) = -17,905\n",
      "Day 64: Reward (in $) = -48,048\n",
      "Day 65: Reward (in $) = 37,292\n",
      "Day 66: Reward (in $) = 72,034\n",
      "Day 67: Reward (in $) = -54,782\n",
      "Day 68: Reward (in $) = -64,241\n",
      "Day 69: Reward (in $) = -28,745\n",
      "Day 70: Reward (in $) = 137,398\n",
      "Day 71: Reward (in $) = -6,018\n",
      "Day 72: Reward (in $) = -8,710\n",
      "Day 73: Reward (in $) = -53,550\n",
      "Day 74: Reward (in $) = -33,943\n",
      "Day 75: Reward (in $) = -3,837\n",
      "Day 76: Reward (in $) = -95,146\n",
      "Day 77: Reward (in $) = -15,434\n",
      "Day 78: Reward (in $) = -34,871\n",
      "Day 79: Reward (in $) = -40,891\n",
      "Day 80: Reward (in $) = -38,052\n",
      "Day 81: Reward (in $) = 111,427\n",
      "Day 82: Reward (in $) = 25,903\n",
      "Day 83: Reward (in $) = -9,661\n",
      "Day 84: Reward (in $) = -3,729\n",
      "Day 85: Reward (in $) = 22,290\n",
      "Day 86: Reward (in $) = -28,981\n",
      "Day 87: Reward (in $) = 2,646,026\n",
      "Day 88: Reward (in $) = 929,907\n",
      "Day 89: Reward (in $) = -25,275\n",
      "Day 90: Reward (in $) = -11,256\n",
      "Day 91: Reward (in $) = -113,396\n",
      "Day 92: Reward (in $) = 75,823\n",
      "Day 93: Reward (in $) = -1,140\n",
      "Day 94: Reward (in $) = 6,071\n",
      "Day 95: Reward (in $) = -24,004\n",
      "Day 96: Reward (in $) = 65,210\n",
      "Day 97: Reward (in $) = 24,311\n",
      "Day 98: Reward (in $) = -6,942\n",
      "Day 99: Reward (in $) = -25,094\n",
      "Day 100: Reward (in $) = -36,201\n",
      "Day 101: Reward (in $) = -19,541\n",
      "Day 102: Reward (in $) = -39,576\n",
      "Day 103: Reward (in $) = 76,728\n",
      "Day 104: Reward (in $) = -33,672\n",
      "Day 105: Reward (in $) = -72,935\n",
      "Day 106: Reward (in $) = -12,414\n",
      "Day 107: Reward (in $) = -24,210\n",
      "Day 108: Reward (in $) = 31,334\n",
      "Day 109: Reward (in $) = -2,578\n",
      "Day 110: Reward (in $) = -40,398\n",
      "Day 111: Reward (in $) = -31,816\n",
      "Day 112: Reward (in $) = 110,562\n",
      "Day 113: Reward (in $) = -48,611\n",
      "Day 114: Reward (in $) = -62,196\n",
      "Day 115: Reward (in $) = -118,117\n",
      "Day 116: Reward (in $) = 7,944\n",
      "Day 117: Reward (in $) = -17,564\n",
      "Day 118: Reward (in $) = 4,897\n",
      "Day 119: Reward (in $) = -9,638\n",
      "Day 120: Reward (in $) = 14,864\n",
      "Day 121: Reward (in $) = 52,348\n",
      "Day 122: Reward (in $) = -28,859\n",
      "Day 123: Reward (in $) = -15,668\n",
      "Day 124: Reward (in $) = -40,464\n",
      "Day 125: Reward (in $) = -31,236\n",
      "Day 126: Reward (in $) = -7,062\n",
      "Day 127: Reward (in $) = -38,986\n",
      "Day 128: Reward (in $) = -32,811\n",
      "Day 129: Reward (in $) = -38,144\n",
      "Day 130: Reward (in $) = -11,255\n",
      "Day 131: Reward (in $) = 13,965\n",
      "Day 132: Reward (in $) = 46,072\n",
      "Day 133: Reward (in $) = 59,097\n",
      "Day 134: Reward (in $) = -49,634\n",
      "Day 135: Reward (in $) = -15,509\n",
      "Day 136: Reward (in $) = -42,377\n",
      "Day 137: Reward (in $) = -47,629\n",
      "Day 138: Reward (in $) = -102,611\n",
      "Day 139: Reward (in $) = -2,322\n",
      "Day 140: Reward (in $) = -53,073\n",
      "Day 141: Reward (in $) = 26,735\n",
      "Day 142: Reward (in $) = 26,028\n",
      "Day 143: Reward (in $) = -12,295\n",
      "Day 144: Reward (in $) = -38,499\n",
      "Day 145: Reward (in $) = -45,913\n",
      "Day 146: Reward (in $) = -7,448\n",
      "Day 147: Reward (in $) = -69,774\n",
      "Day 148: Reward (in $) = -12,960\n",
      "Day 149: Reward (in $) = -7,815\n",
      "Day 150: Reward (in $) = 54,926\n",
      "Day 151: Reward (in $) = 512,056\n",
      "Day 152: Reward (in $) = 381,130\n",
      "Day 153: Reward (in $) = 121,884\n",
      "Day 154: Reward (in $) = 5,484\n",
      "Day 155: Reward (in $) = -44,464\n",
      "Day 156: Reward (in $) = -32,373\n",
      "Day 157: Reward (in $) = -128,972\n",
      "Day 158: Reward (in $) = -74,309\n",
      "Day 159: Reward (in $) = -36,135\n",
      "Day 160: Reward (in $) = -119,695\n",
      "Day 161: Reward (in $) = -45,421\n",
      "Day 162: Reward (in $) = 436,694\n",
      "Day 163: Reward (in $) = -14,904\n",
      "Day 164: Reward (in $) = -13,095\n",
      "Day 165: Reward (in $) = 618,612\n",
      "Day 166: Reward (in $) = -20,755\n",
      "Day 167: Reward (in $) = 8,751\n",
      "Day 168: Reward (in $) = 1,019\n",
      "Day 169: Reward (in $) = 7,625\n",
      "Day 170: Reward (in $) = -32,784\n",
      "Day 171: Reward (in $) = -4,305\n",
      "Day 172: Reward (in $) = -12,228\n",
      "Day 173: Reward (in $) = 20,743\n",
      "Day 174: Reward (in $) = 15,052\n",
      "Day 175: Reward (in $) = 17,660\n",
      "Day 176: Reward (in $) = -16,308\n",
      "Day 177: Reward (in $) = -28,645\n",
      "Day 178: Reward (in $) = -23,440\n",
      "Day 179: Reward (in $) = -17,475\n",
      "Day 180: Reward (in $) = -562\n",
      "Day 181: Reward (in $) = -20,002\n",
      "Day 182: Reward (in $) = -14,373\n",
      "Day 183: Reward (in $) = -70,158\n",
      "Day 184: Reward (in $) = -101,601\n",
      "Day 185: Reward (in $) = -32,983\n",
      "Day 186: Reward (in $) = 32,797\n",
      "Day 187: Reward (in $) = 15,605\n",
      "Day 188: Reward (in $) = 2,367\n",
      "Day 189: Reward (in $) = -5,984\n",
      "Day 190: Reward (in $) = -99,721\n",
      "Day 191: Reward (in $) = -23,274\n",
      "Day 192: Reward (in $) = -22,736\n",
      "Day 193: Reward (in $) = -61,058\n",
      "Day 194: Reward (in $) = -701\n",
      "Day 195: Reward (in $) = 6,112\n",
      "Day 196: Reward (in $) = 10,735\n",
      "Day 197: Reward (in $) = 12,011\n",
      "Day 198: Reward (in $) = 38,449\n",
      "Day 199: Reward (in $) = -5,150\n",
      "Day 200: Reward (in $) = -77,585\n",
      "Day 201: Reward (in $) = -7,309\n",
      "Day 202: Reward (in $) = -5,605\n",
      "Day 203: Reward (in $) = 14,922\n",
      "Day 204: Reward (in $) = 4,816\n",
      "Day 205: Reward (in $) = -26,033\n",
      "Day 206: Reward (in $) = -2,365\n",
      "Day 207: Reward (in $) = -4,315\n",
      "Day 208: Reward (in $) = 21,593\n",
      "Day 209: Reward (in $) = 23,217\n",
      "Day 210: Reward (in $) = 205,777\n",
      "Day 211: Reward (in $) = 47,414\n",
      "Day 212: Reward (in $) = 54,924\n",
      "Day 213: Reward (in $) = -6,129\n",
      "Day 214: Reward (in $) = -69,112\n",
      "Day 215: Reward (in $) = -71,389\n",
      "Day 216: Reward (in $) = 1,043\n",
      "Day 217: Reward (in $) = 1,801\n",
      "Day 218: Reward (in $) = 27,131\n",
      "Day 219: Reward (in $) = -29,409\n",
      "Day 220: Reward (in $) = -4,558\n",
      "Day 221: Reward (in $) = -28,728\n",
      "Day 222: Reward (in $) = 16,299\n",
      "Day 223: Reward (in $) = 5,067\n",
      "Day 224: Reward (in $) = -127\n",
      "Day 225: Reward (in $) = 627\n",
      "Day 226: Reward (in $) = -39,870\n",
      "Day 227: Reward (in $) = -1,546\n",
      "Day 228: Reward (in $) = 15,868\n",
      "Day 229: Reward (in $) = -5,331\n",
      "Day 230: Reward (in $) = -3,249\n",
      "Day 231: Reward (in $) = -1,847\n",
      "Day 232: Reward (in $) = -64,157\n",
      "Day 233: Reward (in $) = -51,090\n",
      "Day 234: Reward (in $) = -80,247\n",
      "Day 235: Reward (in $) = -130,943\n",
      "Day 236: Reward (in $) = 17,443\n",
      "Day 237: Reward (in $) = 56,728\n",
      "Day 238: Reward (in $) = -2,487\n",
      "Day 239: Reward (in $) = -39,735\n",
      "Day 240: Reward (in $) = -16,021\n",
      "Day 241: Reward (in $) = -559\n",
      "Day 242: Reward (in $) = 53,856\n",
      "Day 243: Reward (in $) = -1,849\n",
      "Day 244: Reward (in $) = 6,820\n",
      "Day 245: Reward (in $) = -68,250\n",
      "Day 246: Reward (in $) = -37,242\n",
      "Day 247: Reward (in $) = -5,675\n",
      "Day 248: Reward (in $) = -33,677\n",
      "Day 249: Reward (in $) = -815\n",
      "Day 250: Reward (in $) = -19,086\n",
      "Day 251: Reward (in $) = 411\n",
      "Day 252: Reward (in $) = -402\n",
      "Day 253: Reward (in $) = 139,566\n",
      "Day 254: Reward (in $) = -12,585\n",
      "Day 255: Reward (in $) = -59,308\n",
      "Day 256: Reward (in $) = -3,849\n",
      "Day 257: Reward (in $) = 35,890\n",
      "Day 258: Reward (in $) = -23,556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 259: Reward (in $) = -39,138\n",
      "Day 260: Reward (in $) = -31,092\n",
      "Day 261: Reward (in $) = 68,688\n",
      "Day 262: Reward (in $) = -8,606\n",
      "Day 263: Reward (in $) = -61,880\n",
      "Day 264: Reward (in $) = -40,526\n",
      "Day 265: Reward (in $) = -4,663\n",
      "Day 266: Reward (in $) = -87,739\n",
      "Day 267: Reward (in $) = -8,703\n",
      "Day 268: Reward (in $) = -22,362\n",
      "Day 269: Reward (in $) = -84,037\n",
      "Day 270: Reward (in $) = -70,217\n",
      "Day 271: Reward (in $) = -8,869\n",
      "Day 272: Reward (in $) = -25,087\n",
      "Day 273: Reward (in $) = -42,839\n",
      "Day 274: Reward (in $) = 22,621\n",
      "Day 275: Reward (in $) = 23,373\n",
      "Day 276: Reward (in $) = -20,355\n",
      "Day 277: Reward (in $) = 73,821\n",
      "Day 278: Reward (in $) = 141,076\n",
      "Day 279: Reward (in $) = 803\n",
      "Day 280: Reward (in $) = -26,410\n",
      "Day 281: Reward (in $) = -11,458\n",
      "Day 282: Reward (in $) = 12,581\n",
      "Day 283: Reward (in $) = -5,498\n",
      "Day 284: Reward (in $) = -33,155\n",
      "Day 285: Reward (in $) = 17,325\n",
      "Day 286: Reward (in $) = -54,204\n",
      "Day 287: Reward (in $) = -61,896\n",
      "Day 288: Reward (in $) = 4,166\n",
      "Day 289: Reward (in $) = 5,426\n",
      "Day 290: Reward (in $) = 8,915\n",
      "Day 291: Reward (in $) = -6,449\n",
      "Day 292: Reward (in $) = -1,680\n",
      "Day 293: Reward (in $) = 71,649\n",
      "Day 294: Reward (in $) = -11,242\n",
      "Day 295: Reward (in $) = 6,043\n",
      "Day 296: Reward (in $) = -49,684\n",
      "Day 297: Reward (in $) = -33,199\n",
      "Day 298: Reward (in $) = 131,265\n",
      "Day 299: Reward (in $) = -61,398\n",
      "Day 300: Reward (in $) = -16,194\n",
      "Day 301: Reward (in $) = -7,995\n",
      "Day 302: Reward (in $) = -21,787\n",
      "Day 303: Reward (in $) = 13,647\n",
      "Day 304: Reward (in $) = -53,666\n",
      "Day 305: Reward (in $) = -15,326\n",
      "Day 306: Reward (in $) = -16,967\n",
      "Day 307: Reward (in $) = -6,787\n",
      "Day 308: Reward (in $) = -44,752\n",
      "Day 309: Reward (in $) = 2,760\n",
      "Day 310: Reward (in $) = -5,012\n",
      "Day 311: Reward (in $) = -33,082\n",
      "Day 312: Reward (in $) = 43,807\n",
      "Day 313: Reward (in $) = -1,454\n",
      "Day 314: Reward (in $) = -59,596\n",
      "Day 315: Reward (in $) = -48,723\n",
      "Day 316: Reward (in $) = -11,707\n",
      "Day 317: Reward (in $) = -33,945\n",
      "Day 318: Reward (in $) = -17,345\n",
      "Day 319: Reward (in $) = 2,558\n",
      "Day 320: Reward (in $) = 16,363\n",
      "Day 321: Reward (in $) = 10,440\n",
      "Day 322: Reward (in $) = -5,184\n",
      "Day 323: Reward (in $) = 7,471\n",
      "Day 324: Reward (in $) = 15,927\n",
      "Day 325: Reward (in $) = -26,404\n",
      "Day 326: Reward (in $) = -10,239\n",
      "Day 327: Reward (in $) = -11,869\n",
      "Day 328: Reward (in $) = 58,450\n",
      "Day 329: Reward (in $) = 37,726\n",
      "Day 330: Reward (in $) = 6,195\n",
      "Day 331: Reward (in $) = -11,570\n",
      "Day 332: Reward (in $) = -45,488\n",
      "Day 333: Reward (in $) = -42,086\n",
      "Day 334: Reward (in $) = -2,415\n",
      "Day 335: Reward (in $) = -20,397\n",
      "Day 336: Reward (in $) = -17,812\n",
      "Day 337: Reward (in $) = 18,746\n",
      "Day 338: Reward (in $) = -9,351\n",
      "Day 339: Reward (in $) = -29,189\n",
      "Day 340: Reward (in $) = -28,881\n",
      "Day 341: Reward (in $) = -51,540\n",
      "Day 342: Reward (in $) = -105,979\n",
      "Day 343: Reward (in $) = -122,682\n",
      "Day 344: Reward (in $) = 165,304\n",
      "Day 345: Reward (in $) = 76,290\n",
      "Day 346: Reward (in $) = -23,490\n",
      "Day 347: Reward (in $) = 7,486\n",
      "Day 348: Reward (in $) = -35,251\n",
      "Day 349: Reward (in $) = 76,995\n",
      "Day 350: Reward (in $) = -4,206\n",
      "Day 351: Reward (in $) = 77,434\n",
      "Day 352: Reward (in $) = 45,188\n",
      "Day 353: Reward (in $) = 9,182\n",
      "Day 354: Reward (in $) = -44,484\n",
      "Day 355: Reward (in $) = -61,024\n",
      "Day 356: Reward (in $) = -23,841\n",
      "Day 357: Reward (in $) = -30,063\n",
      "Day 358: Reward (in $) = -11,839\n",
      "Day 359: Reward (in $) = -11,875\n",
      "Day 360: Reward (in $) = -31,843\n",
      "Day 361: Reward (in $) = -71,100\n",
      "Day 362: Reward (in $) = 25,487\n",
      "Day 363: Reward (in $) = 269,876\n",
      "Day 364: Reward (in $) = -7\n",
      "Day 365: Reward (in $) = -14,433\n",
      "Total money earned over the year (in $) = 3,191,907\n"
     ]
    }
   ],
   "source": [
    "dailyBudget = 250000\n",
    "quantileOffer =0.80\n",
    "\n",
    "# Keep track of your rewards in each day, a cumulative reward over the year, and a total reward over the\n",
    "# year. Also, keep track of the total reward from each option. Store them as dictionaries that are\n",
    "# indexed by day of the year.\n",
    "reward = {}\n",
    "cumulativeReward = {}\n",
    "totalReward = 0\n",
    "optionReturn = dict((option, 0) for option in optionNames)\n",
    "\n",
    "# Implement the trading strategy on each day!\n",
    "\n",
    "for day in rangeOfDays:\n",
    "\n",
    "    reward[day] = 0\n",
    "\n",
    "    # Find the options that your classifier says that should be profitable. Store the profitable option\n",
    "    # names in chosenOptions.\n",
    "    chosenOptionNumbers = np.ravel(list(np.nonzero(predictedY.loc[day].values)))\n",
    "    if np.size(chosenOptionNumbers) == 0:\n",
    "        continue\n",
    "    chosenOptions = [optionNames[i] for i in chosenOptionNumbers]\n",
    "\n",
    "    # Design the portfolio based on average price spreads. Our strategy is that as long as you have not\n",
    "    # exceeded your daily budget, pick an option from the list of 'chosenOptions' probabilistically, where\n",
    "    # the probability of choosing it is proportional to exponential(historical rewards of that option). That\n",
    "    # is, a historically profitable option is chosen more often than one that is not. Keep sampling and\n",
    "    # decreasing your budget with each bid.\n",
    "\n",
    "    # Start with an empty portfolio.\n",
    "\n",
    "    portfolio = dict((option, 0) for option in chosenOptions)\n",
    "\n",
    "    # Calculate the probabilities of choosing each option among the list 'chosenOptions'. Recall that\n",
    "    # 'chosenOptions' contains the options that your classifier indicates as being profitable.\n",
    "    priceSpreads = [1.0 * averagePriceSpread[option] for option in chosenOptions]\n",
    "    probabilityOptions = [np.exp(p) for p in priceSpreads]\n",
    "    probabilityOptions /= np.sum(probabilityOptions)\n",
    "\n",
    "    # Start with your daily budget.\n",
    "    budget = dailyBudget\n",
    "\n",
    "    # Sampling among the profitable options and bid based on them.\n",
    "    while budget > np.median([offerPrices[quantileOffer][option] for option in chosenOptions]):\n",
    "\n",
    "        optionToBuy = choice(chosenOptions, p=probabilityOptions)\n",
    "\n",
    "        if budget >= offerPrices[quantileOffer][optionToBuy]:\n",
    "            portfolio[optionToBuy] += 1\n",
    "            budget -= offerPrices[quantileOffer][optionToBuy]\n",
    "\n",
    "    # Compute the reward from the day. Go through each of the options you have decided to buy.\n",
    "    # If the DA price is lower than the bid price, then your bid is cleared. For each option you\n",
    "    # have bought, you get a reward equal to the DA - RT price.\n",
    "\n",
    "    for option in chosenOptions:\n",
    "        if testPriceDA.at[day, option] < offerPrices[quantileOffer][option]:\n",
    "            rewardOptionDay = testPriceDART.at[day, option] * portfolio[option]\n",
    "            optionReturn[option] += rewardOptionDay\n",
    "            reward[day] += rewardOptionDay\n",
    "\n",
    "    totalReward += reward[day]\n",
    "\n",
    "    # Calculate the cumulative reward in millions of dollars.\n",
    "    cumulativeReward[day] = totalReward/1000000\n",
    "\n",
    "    print(\"Day \" + str(day) + \": Reward (in $) = \" + \"{0:,.0f}\".format(reward[day]))\n",
    "\n",
    "print(\"Total money earned over the year (in $) = \" + \"{0:,.0f}\".format(totalReward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Quantile (25 points)\n",
    "\n",
    "For the best classifier, try different quanltile choices from the following list: 0.70, 0.75, 0.80, 0.85, 0.90, 0.95. Rank them in terms of profits (low to high). \n",
    "\n",
    "How do you expect the quantile to affect the portfolio? \n",
    "\n",
    "#### (comments here, add a new code cell below)\n",
    "<br>Q = 0.70 : 6,932,325 </br>\n",
    "<br>Q = 0.75 : 9,051,338 </br> \n",
    "<br>Q = 0.80 : 8,136,625 </br>\n",
    "<br>Q = 0.85 : 6,703,334 </br>\n",
    "<br>Q = 0.90 : 4,836,639 </br> \n",
    "<br>Q = 0.95 : 3,940,849 </br> \n",
    "\n",
    "<br> Having a different quantile level determines how high the user will bid given the price is within the previous years quantile region. The higher the quantile level, the more higher the user will bid while at a lower quantile value will yield the person bidding at a lower amount. The highest quantile yielded the lowest value while a quantile level between 0.70 - 0.80 seemed to yield good amounts. This made sense as bidding at a 95% quantile will likely lead having a smaller gap between the equlibirum price and the offered price. Thus we should set our quantile level between 0.7 and 0.8 as they seem to be reasonable quantile levels to bid on. </br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 3: Reward (in $) = -14,447\n",
      "Day 4: Reward (in $) = 1,902,413\n",
      "Day 5: Reward (in $) = 17,363\n",
      "Day 6: Reward (in $) = -41,342\n",
      "Day 7: Reward (in $) = -2,829\n",
      "Day 8: Reward (in $) = -27,956\n",
      "Day 9: Reward (in $) = -18,355\n",
      "Day 10: Reward (in $) = -74,308\n",
      "Day 11: Reward (in $) = -51,577\n",
      "Day 12: Reward (in $) = -47,153\n",
      "Day 13: Reward (in $) = -45,300\n",
      "Day 14: Reward (in $) = -24,659\n",
      "Day 15: Reward (in $) = -5,882\n",
      "Day 16: Reward (in $) = 22,776\n",
      "Day 17: Reward (in $) = -40,329\n",
      "Day 18: Reward (in $) = -18,165\n",
      "Day 19: Reward (in $) = 4,531\n",
      "Day 20: Reward (in $) = -5,144\n",
      "Day 21: Reward (in $) = 16,887\n",
      "Day 22: Reward (in $) = 53,612\n",
      "Day 23: Reward (in $) = 21,422\n",
      "Day 24: Reward (in $) = -53,717\n",
      "Day 25: Reward (in $) = -9,044\n",
      "Day 26: Reward (in $) = -9,728\n",
      "Day 27: Reward (in $) = -5,405\n",
      "Day 28: Reward (in $) = -14,391\n",
      "Day 29: Reward (in $) = -14,973\n",
      "Day 30: Reward (in $) = -30,078\n",
      "Day 31: Reward (in $) = -13,637\n",
      "Day 32: Reward (in $) = -4,991\n",
      "Day 33: Reward (in $) = -44,144\n",
      "Day 34: Reward (in $) = -6,012\n",
      "Day 35: Reward (in $) = 8,134\n",
      "Day 36: Reward (in $) = -20,920\n",
      "Day 37: Reward (in $) = -25,427\n",
      "Day 38: Reward (in $) = 12,536\n",
      "Day 39: Reward (in $) = 7,611\n",
      "Day 40: Reward (in $) = -17,418\n",
      "Day 41: Reward (in $) = -28,094\n",
      "Day 42: Reward (in $) = -8,636\n",
      "Day 43: Reward (in $) = -34,934\n",
      "Day 44: Reward (in $) = -16,559\n",
      "Day 45: Reward (in $) = 30,745\n",
      "Day 46: Reward (in $) = 47,434\n",
      "Day 47: Reward (in $) = 27,734\n",
      "Day 48: Reward (in $) = -34,183\n",
      "Day 49: Reward (in $) = -40,125\n",
      "Day 50: Reward (in $) = -25,974\n",
      "Day 51: Reward (in $) = 3,893\n",
      "Day 52: Reward (in $) = -10,975\n",
      "Day 53: Reward (in $) = -399\n",
      "Day 54: Reward (in $) = -4,434\n",
      "Day 55: Reward (in $) = -2,496\n",
      "Day 56: Reward (in $) = -26,745\n",
      "Day 57: Reward (in $) = -17,239\n",
      "Day 58: Reward (in $) = -11,717\n",
      "Day 59: Reward (in $) = -9,455\n",
      "Day 60: Reward (in $) = -38,028\n",
      "Day 61: Reward (in $) = -94,073\n",
      "Day 62: Reward (in $) = -7,741\n",
      "Day 63: Reward (in $) = -9,207\n",
      "Day 64: Reward (in $) = -2,700\n",
      "Day 65: Reward (in $) = 93,913\n",
      "Day 66: Reward (in $) = 72,570\n",
      "Day 67: Reward (in $) = -19,855\n",
      "Day 68: Reward (in $) = -14,933\n",
      "Day 69: Reward (in $) = -47,319\n",
      "Day 70: Reward (in $) = 46,185\n",
      "Day 71: Reward (in $) = -5,638\n",
      "Day 72: Reward (in $) = -8,625\n",
      "Day 73: Reward (in $) = -68,136\n",
      "Day 74: Reward (in $) = 416,578\n",
      "Day 75: Reward (in $) = -8,494\n",
      "Day 76: Reward (in $) = -56,777\n",
      "Day 77: Reward (in $) = -38,379\n",
      "Day 78: Reward (in $) = -11,715\n",
      "Day 79: Reward (in $) = -13,708\n",
      "Day 80: Reward (in $) = -16,422\n",
      "Day 81: Reward (in $) = -8,992\n",
      "Day 82: Reward (in $) = 7,796\n",
      "Day 83: Reward (in $) = -3,608\n",
      "Day 84: Reward (in $) = -5,172\n",
      "Day 85: Reward (in $) = 11,555\n",
      "Day 86: Reward (in $) = -19,532\n",
      "Day 87: Reward (in $) = 1,873,313\n",
      "Day 88: Reward (in $) = 583,803\n",
      "Day 89: Reward (in $) = -26,831\n",
      "Day 90: Reward (in $) = -39,075\n",
      "Day 91: Reward (in $) = -7,647\n",
      "Day 92: Reward (in $) = -11,200\n",
      "Day 93: Reward (in $) = 1,608\n",
      "Day 94: Reward (in $) = -43,744\n",
      "Day 95: Reward (in $) = -68,099\n",
      "Day 96: Reward (in $) = 56,594\n",
      "Day 97: Reward (in $) = 2,622\n",
      "Day 98: Reward (in $) = -51,577\n",
      "Day 99: Reward (in $) = -58,630\n",
      "Day 100: Reward (in $) = -12,161\n",
      "Day 101: Reward (in $) = -2,232\n",
      "Day 102: Reward (in $) = -93,199\n",
      "Day 103: Reward (in $) = 118,335\n",
      "Day 104: Reward (in $) = -44,320\n",
      "Day 105: Reward (in $) = -34,196\n",
      "Day 106: Reward (in $) = 5,787\n",
      "Day 107: Reward (in $) = -9,544\n",
      "Day 108: Reward (in $) = 14,526\n",
      "Day 109: Reward (in $) = -8,921\n",
      "Day 110: Reward (in $) = -19,981\n",
      "Day 111: Reward (in $) = -16,292\n",
      "Day 112: Reward (in $) = 50,250\n",
      "Day 113: Reward (in $) = -35,315\n",
      "Day 114: Reward (in $) = -38,875\n",
      "Day 115: Reward (in $) = -71,483\n",
      "Day 116: Reward (in $) = -107,231\n",
      "Day 117: Reward (in $) = -21,633\n",
      "Day 118: Reward (in $) = -10,239\n",
      "Day 119: Reward (in $) = -1,479\n",
      "Day 120: Reward (in $) = -3,121\n",
      "Day 121: Reward (in $) = 34,005\n",
      "Day 122: Reward (in $) = -49,128\n",
      "Day 123: Reward (in $) = -37,090\n",
      "Day 124: Reward (in $) = -22,920\n",
      "Day 125: Reward (in $) = -18,252\n",
      "Day 126: Reward (in $) = -41,825\n",
      "Day 127: Reward (in $) = -5,527\n",
      "Day 128: Reward (in $) = -35,498\n",
      "Day 129: Reward (in $) = -53,715\n",
      "Day 130: Reward (in $) = -20,346\n",
      "Day 131: Reward (in $) = 14,583\n",
      "Day 132: Reward (in $) = 80,987\n",
      "Day 133: Reward (in $) = -69,357\n",
      "Day 134: Reward (in $) = -29,145\n",
      "Day 135: Reward (in $) = -7,204\n",
      "Day 136: Reward (in $) = -34,167\n",
      "Day 137: Reward (in $) = -29,305\n",
      "Day 138: Reward (in $) = -5,487\n",
      "Day 139: Reward (in $) = -3,565\n",
      "Day 140: Reward (in $) = -23,875\n",
      "Day 141: Reward (in $) = 9,964\n",
      "Day 142: Reward (in $) = 14,767\n",
      "Day 143: Reward (in $) = 1,642\n",
      "Day 144: Reward (in $) = -36,281\n",
      "Day 145: Reward (in $) = -20,000\n",
      "Day 146: Reward (in $) = -10,810\n",
      "Day 147: Reward (in $) = -26,429\n",
      "Day 148: Reward (in $) = 200,840\n",
      "Day 149: Reward (in $) = 42,592\n",
      "Day 150: Reward (in $) = 32,394\n",
      "Day 151: Reward (in $) = 309,998\n",
      "Day 152: Reward (in $) = 210,586\n",
      "Day 153: Reward (in $) = 38,751\n",
      "Day 154: Reward (in $) = 4,122\n",
      "Day 155: Reward (in $) = -24,525\n",
      "Day 156: Reward (in $) = -26,226\n",
      "Day 157: Reward (in $) = -101,525\n",
      "Day 158: Reward (in $) = -32,231\n",
      "Day 159: Reward (in $) = -17,626\n",
      "Day 160: Reward (in $) = -86,180\n",
      "Day 161: Reward (in $) = 1,147\n",
      "Day 162: Reward (in $) = 54,146\n",
      "Day 163: Reward (in $) = -8,084\n",
      "Day 164: Reward (in $) = -23,696\n",
      "Day 165: Reward (in $) = 86,683\n",
      "Day 166: Reward (in $) = 592,030\n",
      "Day 167: Reward (in $) = 6,909\n",
      "Day 168: Reward (in $) = -38,379\n",
      "Day 169: Reward (in $) = -41,128\n",
      "Day 170: Reward (in $) = -16,996\n",
      "Day 171: Reward (in $) = -38,913\n",
      "Day 172: Reward (in $) = -15,809\n",
      "Day 173: Reward (in $) = -43,862\n",
      "Day 174: Reward (in $) = -37,251\n",
      "Day 175: Reward (in $) = -11,044\n",
      "Day 176: Reward (in $) = -11,095\n",
      "Day 177: Reward (in $) = -42,547\n",
      "Day 178: Reward (in $) = -68,918\n",
      "Day 179: Reward (in $) = -108,760\n",
      "Day 180: Reward (in $) = -53,292\n",
      "Day 181: Reward (in $) = -102,768\n",
      "Day 182: Reward (in $) = -60,126\n",
      "Day 183: Reward (in $) = -53,657\n",
      "Day 184: Reward (in $) = -58,788\n",
      "Day 185: Reward (in $) = -27,899\n",
      "Day 186: Reward (in $) = 40,439\n",
      "Day 187: Reward (in $) = 5,954\n",
      "Day 188: Reward (in $) = -9,418\n",
      "Day 189: Reward (in $) = -6,023\n",
      "Day 190: Reward (in $) = -69,058\n",
      "Day 191: Reward (in $) = -12,105\n",
      "Day 192: Reward (in $) = -16,900\n",
      "Day 193: Reward (in $) = -36,592\n",
      "Day 194: Reward (in $) = -3,046\n",
      "Day 195: Reward (in $) = -14,352\n",
      "Day 196: Reward (in $) = 1,211\n",
      "Day 197: Reward (in $) = 5,474\n",
      "Day 198: Reward (in $) = 66,956\n",
      "Day 199: Reward (in $) = -24,676\n",
      "Day 200: Reward (in $) = -30,040\n",
      "Day 201: Reward (in $) = -54,262\n",
      "Day 202: Reward (in $) = -18,335\n",
      "Day 203: Reward (in $) = 3,292\n",
      "Day 204: Reward (in $) = 2,693\n",
      "Day 205: Reward (in $) = -48,131\n",
      "Day 206: Reward (in $) = -4,488\n",
      "Day 207: Reward (in $) = -7,590\n",
      "Day 208: Reward (in $) = -968\n",
      "Day 209: Reward (in $) = -6,431\n",
      "Day 210: Reward (in $) = 74,771\n",
      "Day 211: Reward (in $) = 29,065\n",
      "Day 212: Reward (in $) = 42,194\n",
      "Day 213: Reward (in $) = -106,675\n",
      "Day 214: Reward (in $) = -35,763\n",
      "Day 215: Reward (in $) = -13,635\n",
      "Day 216: Reward (in $) = -12,408\n",
      "Day 217: Reward (in $) = -21,697\n",
      "Day 218: Reward (in $) = 15,730\n",
      "Day 219: Reward (in $) = -18,914\n",
      "Day 220: Reward (in $) = -59,188\n",
      "Day 221: Reward (in $) = -8,698\n",
      "Day 222: Reward (in $) = 19,985\n",
      "Day 223: Reward (in $) = 43,728\n",
      "Day 224: Reward (in $) = 12,046\n",
      "Day 225: Reward (in $) = 39,158\n",
      "Day 226: Reward (in $) = -29,525\n",
      "Day 227: Reward (in $) = 41,503\n",
      "Day 228: Reward (in $) = 296,434\n",
      "Day 229: Reward (in $) = -7,087\n",
      "Day 230: Reward (in $) = -3,200\n",
      "Day 231: Reward (in $) = 4,966\n",
      "Day 232: Reward (in $) = -32,738\n",
      "Day 233: Reward (in $) = -31,111\n",
      "Day 234: Reward (in $) = -64,830\n",
      "Day 235: Reward (in $) = -84,356\n",
      "Day 236: Reward (in $) = -8,199\n",
      "Day 237: Reward (in $) = 26,064\n",
      "Day 238: Reward (in $) = -10,367\n",
      "Day 239: Reward (in $) = -35,546\n",
      "Day 240: Reward (in $) = -59,458\n",
      "Day 241: Reward (in $) = -34,091\n",
      "Day 242: Reward (in $) = -50,836\n",
      "Day 243: Reward (in $) = -81,230\n",
      "Day 244: Reward (in $) = -28,439\n",
      "Day 245: Reward (in $) = -61,995\n",
      "Day 246: Reward (in $) = -30,536\n",
      "Day 247: Reward (in $) = -2,807\n",
      "Day 248: Reward (in $) = -11,233\n",
      "Day 249: Reward (in $) = -9,642\n",
      "Day 250: Reward (in $) = -40,616\n",
      "Day 251: Reward (in $) = 49,314\n",
      "Day 252: Reward (in $) = -12,380\n",
      "Day 253: Reward (in $) = 54,651\n",
      "Day 254: Reward (in $) = -5,601\n",
      "Day 255: Reward (in $) = -88,626\n",
      "Day 256: Reward (in $) = -108,953\n",
      "Day 257: Reward (in $) = 1,519,115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 258: Reward (in $) = -6,595\n",
      "Day 259: Reward (in $) = -20,504\n",
      "Day 260: Reward (in $) = -10,876\n",
      "Day 261: Reward (in $) = 9,579\n",
      "Day 262: Reward (in $) = -1,415\n",
      "Day 263: Reward (in $) = -32,770\n",
      "Day 264: Reward (in $) = -65,739\n",
      "Day 265: Reward (in $) = -4,685\n",
      "Day 266: Reward (in $) = -52,872\n",
      "Day 267: Reward (in $) = -4,928\n",
      "Day 268: Reward (in $) = -14,294\n",
      "Day 269: Reward (in $) = -51,681\n",
      "Day 270: Reward (in $) = -51,268\n",
      "Day 271: Reward (in $) = -5,381\n",
      "Day 272: Reward (in $) = -15,030\n",
      "Day 273: Reward (in $) = -15,101\n",
      "Day 274: Reward (in $) = 6,287\n",
      "Day 275: Reward (in $) = 14,469\n",
      "Day 276: Reward (in $) = 25,001\n",
      "Day 277: Reward (in $) = 68,543\n",
      "Day 278: Reward (in $) = 108,481\n",
      "Day 279: Reward (in $) = 22,984\n",
      "Day 280: Reward (in $) = 5,119\n",
      "Day 281: Reward (in $) = -4,461\n",
      "Day 282: Reward (in $) = -1,522\n",
      "Day 283: Reward (in $) = -80,715\n",
      "Day 284: Reward (in $) = 8,969\n",
      "Day 285: Reward (in $) = -35,134\n",
      "Day 286: Reward (in $) = -28,449\n",
      "Day 287: Reward (in $) = -23,347\n",
      "Day 288: Reward (in $) = 7,763\n",
      "Day 289: Reward (in $) = 15,710\n",
      "Day 290: Reward (in $) = -47,379\n",
      "Day 291: Reward (in $) = -21,095\n",
      "Day 292: Reward (in $) = -10,536\n",
      "Day 293: Reward (in $) = -6,265\n",
      "Day 294: Reward (in $) = 16,179\n",
      "Day 295: Reward (in $) = -4,045\n",
      "Day 296: Reward (in $) = 358\n",
      "Day 297: Reward (in $) = -22,274\n",
      "Day 298: Reward (in $) = 275,059\n",
      "Day 299: Reward (in $) = -2,776\n",
      "Day 300: Reward (in $) = -30,177\n",
      "Day 301: Reward (in $) = -5,524\n",
      "Day 302: Reward (in $) = -2,747\n",
      "Day 303: Reward (in $) = 1,814\n",
      "Day 304: Reward (in $) = -55,528\n",
      "Day 305: Reward (in $) = -8,926\n",
      "Day 306: Reward (in $) = -4,064\n",
      "Day 307: Reward (in $) = 82,992\n",
      "Day 308: Reward (in $) = -30,674\n",
      "Day 309: Reward (in $) = -11,748\n",
      "Day 310: Reward (in $) = -4,616\n",
      "Day 311: Reward (in $) = -20,569\n",
      "Day 312: Reward (in $) = 27,670\n",
      "Day 313: Reward (in $) = -1,421\n",
      "Day 314: Reward (in $) = -18,992\n",
      "Day 315: Reward (in $) = -30,375\n",
      "Day 316: Reward (in $) = -11,115\n",
      "Day 317: Reward (in $) = -18,138\n",
      "Day 318: Reward (in $) = 36,321\n",
      "Day 319: Reward (in $) = 1,659\n",
      "Day 320: Reward (in $) = 9,683\n",
      "Day 321: Reward (in $) = 12,306\n",
      "Day 322: Reward (in $) = 1,317\n",
      "Day 323: Reward (in $) = 3,874\n",
      "Day 324: Reward (in $) = 9,614\n",
      "Day 325: Reward (in $) = -13,112\n",
      "Day 326: Reward (in $) = -7,569\n",
      "Day 327: Reward (in $) = 12,621\n",
      "Day 328: Reward (in $) = 68,412\n",
      "Day 329: Reward (in $) = 21,800\n",
      "Day 330: Reward (in $) = 3,972\n",
      "Day 331: Reward (in $) = -559\n",
      "Day 332: Reward (in $) = -30,000\n",
      "Day 333: Reward (in $) = -49,512\n",
      "Day 334: Reward (in $) = -2,436\n",
      "Day 335: Reward (in $) = -11,178\n",
      "Day 336: Reward (in $) = -15,226\n",
      "Day 337: Reward (in $) = 6,444\n",
      "Day 338: Reward (in $) = -6,050\n",
      "Day 339: Reward (in $) = -20,898\n",
      "Day 340: Reward (in $) = -50,654\n",
      "Day 341: Reward (in $) = -26,303\n",
      "Day 342: Reward (in $) = 19,274\n",
      "Day 343: Reward (in $) = 24,204\n",
      "Day 344: Reward (in $) = 37,282\n",
      "Day 345: Reward (in $) = 52,975\n",
      "Day 346: Reward (in $) = -21,288\n",
      "Day 347: Reward (in $) = 12,815\n",
      "Day 348: Reward (in $) = -21,033\n",
      "Day 349: Reward (in $) = 38,014\n",
      "Day 350: Reward (in $) = 9,673\n",
      "Day 351: Reward (in $) = 81,436\n",
      "Day 352: Reward (in $) = 8,022\n",
      "Day 353: Reward (in $) = -16,256\n",
      "Day 354: Reward (in $) = -29,005\n",
      "Day 355: Reward (in $) = 9,174\n",
      "Day 356: Reward (in $) = -9,000\n",
      "Day 357: Reward (in $) = -60,077\n",
      "Day 358: Reward (in $) = -13,387\n",
      "Day 359: Reward (in $) = -6,796\n",
      "Day 360: Reward (in $) = -20,606\n",
      "Day 361: Reward (in $) = -47,914\n",
      "Day 362: Reward (in $) = 10,227\n",
      "Day 363: Reward (in $) = 79,798\n",
      "Day 364: Reward (in $) = 314\n",
      "Day 365: Reward (in $) = -16,743\n",
      "Total money earned over the year (in $) = 3,940,849\n"
     ]
    }
   ],
   "source": [
    "dailyBudget = 250000\n",
    "quantileOffer = 0.95\n",
    "\n",
    "# Keep track of your rewards in each day, a cumulative reward over the year, and a total reward over the\n",
    "# year. Also, keep track of the total reward from each option. Store them as dictionaries that are\n",
    "# indexed by day of the year.\n",
    "reward = {}\n",
    "cumulativeReward = {}\n",
    "totalReward = 0\n",
    "optionReturn = dict((option, 0) for option in optionNames)\n",
    "\n",
    "# Implement the trading strategy on each day!\n",
    "\n",
    "for day in rangeOfDays:\n",
    "\n",
    "    reward[day] = 0\n",
    "\n",
    "    # Find the options that your classifier says that should be profitable. Store the profitable option\n",
    "    # names in chosenOptions.\n",
    "    chosenOptionNumbers = np.ravel(list(np.nonzero(predictedY.loc[day].values)))\n",
    "    if np.size(chosenOptionNumbers) == 0:\n",
    "        continue\n",
    "    chosenOptions = [optionNames[i] for i in chosenOptionNumbers]\n",
    "\n",
    "    # Design the portfolio based on average price spreads. Our strategy is that as long as you have not\n",
    "    # exceeded your daily budget, pick an option from the list of 'chosenOptions' probabilistically, where\n",
    "    # the probability of choosing it is proportional to exponential(historical rewards of that option). That\n",
    "    # is, a historically profitable option is chosen more often than one that is not. Keep sampling and\n",
    "    # decreasing your budget with each bid.\n",
    "\n",
    "    # Start with an empty portfolio.\n",
    "\n",
    "    portfolio = dict((option, 0) for option in chosenOptions)\n",
    "\n",
    "    # Calculate the probabilities of choosing each option among the list 'chosenOptions'. Recall that\n",
    "    # 'chosenOptions' contains the options that your classifier indicates as being profitable.\n",
    "    priceSpreads = [1.0 * averagePriceSpread[option] for option in chosenOptions]\n",
    "    probabilityOptions = [np.exp(p) for p in priceSpreads]\n",
    "    probabilityOptions /= np.sum(probabilityOptions)\n",
    "\n",
    "    # Start with your daily budget.\n",
    "    budget = dailyBudget\n",
    "\n",
    "    # Sampling among the profitable options and bid based on them.\n",
    "    while budget > np.median([offerPrices[quantileOffer][option] for option in chosenOptions]):\n",
    "\n",
    "        optionToBuy = choice(chosenOptions, p=probabilityOptions)\n",
    "\n",
    "        if budget >= offerPrices[quantileOffer][optionToBuy]:\n",
    "            portfolio[optionToBuy] += 1\n",
    "            budget -= offerPrices[quantileOffer][optionToBuy]\n",
    "\n",
    "    # Compute the reward from the day. Go through each of the options you have decided to buy.\n",
    "    # If the DA price is lower than the bid price, then your bid is cleared. For each option you\n",
    "    # have bought, you get a reward equal to the DA - RT price.\n",
    "\n",
    "    for option in chosenOptions:\n",
    "        if testPriceDA.at[day, option] < offerPrices[quantileOffer][option]:\n",
    "            rewardOptionDay = testPriceDART.at[day, option] * portfolio[option]\n",
    "            optionReturn[option] += rewardOptionDay\n",
    "            reward[day] += rewardOptionDay\n",
    "\n",
    "    totalReward += reward[day]\n",
    "\n",
    "    # Calculate the cumulative reward in millions of dollars.\n",
    "    cumulativeReward[day] = totalReward/1000000\n",
    "\n",
    "    print(\"Day \" + str(day) + \": Reward (in $) = \" + \"{0:,.0f}\".format(reward[day]))\n",
    "\n",
    "print(\"Total money earned over the year (in $) = \" + \"{0:,.0f}\".format(totalReward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Daily budget (25 points)\n",
    "\n",
    "We used a daily budget of 250,000. Try different budgets, let's say, 100,000, 150,000, 200,000, and 300,000. Rank them in terms of profits (low to high). Fix the quantile to the one that led to maximum profits in the previous task. \n",
    "\n",
    "#### (comments here, add a new code cell below)\n",
    "<br>Daily Budget = 100,000 : 3,632,160</br> \n",
    "<br>Daily Budget = 150,000 : 5,456,777</br>\n",
    "<br>Daily Budget = 200,000 : 7,228,041</br>\n",
    "<br>Daily Budget = 300,000 : 10,899,459</br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 3: Reward (in $) = -88,069\n",
      "Day 4: Reward (in $) = 3,894,773\n",
      "Day 5: Reward (in $) = 31,218\n",
      "Day 6: Reward (in $) = -59,442\n",
      "Day 7: Reward (in $) = -3,356\n",
      "Day 8: Reward (in $) = -56,261\n",
      "Day 9: Reward (in $) = -38,559\n",
      "Day 10: Reward (in $) = -144,120\n",
      "Day 11: Reward (in $) = -99,857\n",
      "Day 12: Reward (in $) = -103,065\n",
      "Day 13: Reward (in $) = -85,624\n",
      "Day 14: Reward (in $) = -51,712\n",
      "Day 15: Reward (in $) = -12,276\n",
      "Day 16: Reward (in $) = 46,067\n",
      "Day 17: Reward (in $) = -83,714\n",
      "Day 18: Reward (in $) = -34,674\n",
      "Day 19: Reward (in $) = 4,453\n",
      "Day 20: Reward (in $) = -8,682\n",
      "Day 21: Reward (in $) = 34,920\n",
      "Day 22: Reward (in $) = 107,150\n",
      "Day 23: Reward (in $) = 36,330\n",
      "Day 24: Reward (in $) = -102,756\n",
      "Day 25: Reward (in $) = -35,203\n",
      "Day 26: Reward (in $) = -22,455\n",
      "Day 27: Reward (in $) = -15,325\n",
      "Day 28: Reward (in $) = -27,644\n",
      "Day 29: Reward (in $) = -30,371\n",
      "Day 30: Reward (in $) = -78,556\n",
      "Day 31: Reward (in $) = -34,170\n",
      "Day 32: Reward (in $) = -10,426\n",
      "Day 33: Reward (in $) = -89,519\n",
      "Day 34: Reward (in $) = -11,723\n",
      "Day 35: Reward (in $) = 21,118\n",
      "Day 36: Reward (in $) = -44,183\n",
      "Day 37: Reward (in $) = -47,982\n",
      "Day 38: Reward (in $) = 25,874\n",
      "Day 39: Reward (in $) = 16,227\n",
      "Day 40: Reward (in $) = -48,263\n",
      "Day 41: Reward (in $) = -55,922\n",
      "Day 42: Reward (in $) = -17,883\n",
      "Day 43: Reward (in $) = -63,459\n",
      "Day 44: Reward (in $) = -32,080\n",
      "Day 45: Reward (in $) = 59,790\n",
      "Day 46: Reward (in $) = 90,733\n",
      "Day 47: Reward (in $) = 57,437\n",
      "Day 48: Reward (in $) = -66,449\n",
      "Day 49: Reward (in $) = -75,202\n",
      "Day 50: Reward (in $) = -70,158\n",
      "Day 51: Reward (in $) = 13,730\n",
      "Day 52: Reward (in $) = -24,185\n",
      "Day 53: Reward (in $) = -131\n",
      "Day 54: Reward (in $) = -8,492\n",
      "Day 55: Reward (in $) = -5,572\n",
      "Day 56: Reward (in $) = -55,733\n",
      "Day 57: Reward (in $) = -35,785\n",
      "Day 58: Reward (in $) = -24,029\n",
      "Day 59: Reward (in $) = -19,890\n",
      "Day 60: Reward (in $) = -77,544\n",
      "Day 61: Reward (in $) = -193,391\n",
      "Day 62: Reward (in $) = -4,064\n",
      "Day 63: Reward (in $) = -13,526\n",
      "Day 64: Reward (in $) = -3,602\n",
      "Day 65: Reward (in $) = 183,064\n",
      "Day 66: Reward (in $) = 175,484\n",
      "Day 67: Reward (in $) = -14,882\n",
      "Day 68: Reward (in $) = -52,053\n",
      "Day 69: Reward (in $) = -127,132\n",
      "Day 70: Reward (in $) = 102,024\n",
      "Day 71: Reward (in $) = -12,082\n",
      "Day 72: Reward (in $) = -16,711\n",
      "Day 73: Reward (in $) = -149,502\n",
      "Day 74: Reward (in $) = 922,557\n",
      "Day 75: Reward (in $) = -21,612\n",
      "Day 76: Reward (in $) = -117,186\n",
      "Day 77: Reward (in $) = -93,027\n",
      "Day 78: Reward (in $) = -33,253\n",
      "Day 79: Reward (in $) = -32,698\n",
      "Day 80: Reward (in $) = -30,602\n",
      "Day 81: Reward (in $) = -17,946\n",
      "Day 82: Reward (in $) = 13,679\n",
      "Day 83: Reward (in $) = -12,959\n",
      "Day 84: Reward (in $) = -14,017\n",
      "Day 85: Reward (in $) = 30,969\n",
      "Day 86: Reward (in $) = -41,382\n",
      "Day 87: Reward (in $) = 3,778,189\n",
      "Day 88: Reward (in $) = 1,160,612\n",
      "Day 89: Reward (in $) = -62,872\n",
      "Day 90: Reward (in $) = -17,824\n",
      "Day 91: Reward (in $) = -19,483\n",
      "Day 92: Reward (in $) = -37,767\n",
      "Day 93: Reward (in $) = 3,385\n",
      "Day 94: Reward (in $) = 15,950\n",
      "Day 95: Reward (in $) = -7,244\n",
      "Day 96: Reward (in $) = 84,602\n",
      "Day 97: Reward (in $) = 17,324\n",
      "Day 98: Reward (in $) = -99,588\n",
      "Day 99: Reward (in $) = -39,366\n",
      "Day 100: Reward (in $) = -23,167\n",
      "Day 101: Reward (in $) = 2,701\n",
      "Day 102: Reward (in $) = -166\n",
      "Day 103: Reward (in $) = 253,790\n",
      "Day 104: Reward (in $) = -46,115\n",
      "Day 105: Reward (in $) = -8,082\n",
      "Day 106: Reward (in $) = 20,477\n",
      "Day 107: Reward (in $) = -18,496\n",
      "Day 108: Reward (in $) = 56,397\n",
      "Day 109: Reward (in $) = -26,391\n",
      "Day 110: Reward (in $) = -54,273\n",
      "Day 111: Reward (in $) = -40,939\n",
      "Day 112: Reward (in $) = 95,120\n",
      "Day 113: Reward (in $) = -72,870\n",
      "Day 114: Reward (in $) = -72,710\n",
      "Day 115: Reward (in $) = -155,325\n",
      "Day 116: Reward (in $) = 994\n",
      "Day 117: Reward (in $) = -47,576\n",
      "Day 118: Reward (in $) = -20,592\n",
      "Day 119: Reward (in $) = -4,052\n",
      "Day 120: Reward (in $) = -6,075\n",
      "Day 121: Reward (in $) = 70,807\n",
      "Day 122: Reward (in $) = -54,385\n",
      "Day 123: Reward (in $) = -84,152\n",
      "Day 124: Reward (in $) = -57,280\n",
      "Day 125: Reward (in $) = -39,912\n",
      "Day 126: Reward (in $) = -84,965\n",
      "Day 127: Reward (in $) = -18,924\n",
      "Day 128: Reward (in $) = -70,918\n",
      "Day 129: Reward (in $) = -122,952\n",
      "Day 130: Reward (in $) = -48,325\n",
      "Day 131: Reward (in $) = 29,807\n",
      "Day 132: Reward (in $) = 171,636\n",
      "Day 133: Reward (in $) = -139,221\n",
      "Day 134: Reward (in $) = -63,606\n",
      "Day 135: Reward (in $) = -14,479\n",
      "Day 136: Reward (in $) = -71,515\n",
      "Day 137: Reward (in $) = -58,415\n",
      "Day 138: Reward (in $) = -14,065\n",
      "Day 139: Reward (in $) = -7,729\n",
      "Day 140: Reward (in $) = -48,598\n",
      "Day 141: Reward (in $) = 23,637\n",
      "Day 142: Reward (in $) = 30,032\n",
      "Day 143: Reward (in $) = 5,238\n",
      "Day 144: Reward (in $) = -76,176\n",
      "Day 145: Reward (in $) = -66,432\n",
      "Day 146: Reward (in $) = -18,583\n",
      "Day 147: Reward (in $) = -83,725\n",
      "Day 148: Reward (in $) = 414,051\n",
      "Day 149: Reward (in $) = 88,954\n",
      "Day 150: Reward (in $) = 68,402\n",
      "Day 151: Reward (in $) = 757,612\n",
      "Day 152: Reward (in $) = 466,386\n",
      "Day 153: Reward (in $) = 80,556\n",
      "Day 154: Reward (in $) = 10,406\n",
      "Day 155: Reward (in $) = -54,998\n",
      "Day 156: Reward (in $) = -58,783\n",
      "Day 157: Reward (in $) = -107,943\n",
      "Day 158: Reward (in $) = -97,165\n",
      "Day 159: Reward (in $) = -46,351\n",
      "Day 160: Reward (in $) = -172,965\n",
      "Day 161: Reward (in $) = 3,217\n",
      "Day 162: Reward (in $) = 190,397\n",
      "Day 163: Reward (in $) = -16,329\n",
      "Day 164: Reward (in $) = -60,844\n",
      "Day 165: Reward (in $) = 189,972\n",
      "Day 166: Reward (in $) = 12,244\n",
      "Day 167: Reward (in $) = 36,695\n",
      "Day 168: Reward (in $) = -1,664\n",
      "Day 169: Reward (in $) = 13,173\n",
      "Day 170: Reward (in $) = -50,268\n",
      "Day 171: Reward (in $) = -85,154\n",
      "Day 172: Reward (in $) = -5,702\n",
      "Day 173: Reward (in $) = -1,397\n",
      "Day 174: Reward (in $) = 11,359\n",
      "Day 175: Reward (in $) = 38,249\n",
      "Day 176: Reward (in $) = -23,790\n",
      "Day 177: Reward (in $) = -64,640\n",
      "Day 178: Reward (in $) = -8,693\n",
      "Day 179: Reward (in $) = -4,226\n",
      "Day 180: Reward (in $) = -49,449\n",
      "Day 181: Reward (in $) = -27,599\n",
      "Day 182: Reward (in $) = -89,272\n",
      "Day 183: Reward (in $) = -102,465\n",
      "Day 184: Reward (in $) = -114,867\n",
      "Day 185: Reward (in $) = -57,584\n",
      "Day 186: Reward (in $) = 107,718\n",
      "Day 187: Reward (in $) = 8,587\n",
      "Day 188: Reward (in $) = 91\n",
      "Day 189: Reward (in $) = -188\n",
      "Day 190: Reward (in $) = -135,592\n",
      "Day 191: Reward (in $) = -33,845\n",
      "Day 192: Reward (in $) = -41,570\n",
      "Day 193: Reward (in $) = -73,713\n",
      "Day 194: Reward (in $) = -590\n",
      "Day 195: Reward (in $) = 2,033\n",
      "Day 196: Reward (in $) = 9,479\n",
      "Day 197: Reward (in $) = 13,025\n",
      "Day 198: Reward (in $) = 117,622\n",
      "Day 199: Reward (in $) = 203\n",
      "Day 200: Reward (in $) = -55,366\n",
      "Day 201: Reward (in $) = -14,502\n",
      "Day 202: Reward (in $) = -86\n",
      "Day 203: Reward (in $) = 326\n",
      "Day 204: Reward (in $) = -17,792\n",
      "Day 205: Reward (in $) = -90,156\n",
      "Day 206: Reward (in $) = -1,835\n",
      "Day 207: Reward (in $) = -5,263\n",
      "Day 208: Reward (in $) = 20\n",
      "Day 209: Reward (in $) = 9,222\n",
      "Day 210: Reward (in $) = 19,822\n",
      "Day 211: Reward (in $) = 63,621\n",
      "Day 212: Reward (in $) = 107,454\n",
      "Day 213: Reward (in $) = -5,614\n",
      "Day 214: Reward (in $) = -6,004\n",
      "Day 215: Reward (in $) = -26,504\n",
      "Day 216: Reward (in $) = 6,764\n",
      "Day 217: Reward (in $) = 2,862\n",
      "Day 218: Reward (in $) = 32,592\n",
      "Day 219: Reward (in $) = -36,808\n",
      "Day 220: Reward (in $) = -3,836\n",
      "Day 221: Reward (in $) = -26,467\n",
      "Day 222: Reward (in $) = 20,714\n",
      "Day 223: Reward (in $) = 1,785\n",
      "Day 224: Reward (in $) = -7\n",
      "Day 225: Reward (in $) = 250\n",
      "Day 226: Reward (in $) = -58,020\n",
      "Day 227: Reward (in $) = -550\n",
      "Day 228: Reward (in $) = 1,632\n",
      "Day 229: Reward (in $) = 237\n",
      "Day 230: Reward (in $) = -3,413\n",
      "Day 231: Reward (in $) = 178\n",
      "Day 232: Reward (in $) = -60,367\n",
      "Day 233: Reward (in $) = -64,136\n",
      "Day 234: Reward (in $) = -132,689\n",
      "Day 235: Reward (in $) = -10,834\n",
      "Day 236: Reward (in $) = -31,814\n",
      "Day 237: Reward (in $) = 69,120\n",
      "Day 238: Reward (in $) = -11,896\n",
      "Day 239: Reward (in $) = -67,482\n",
      "Day 240: Reward (in $) = -2,737\n",
      "Day 241: Reward (in $) = -1,457\n",
      "Day 242: Reward (in $) = -110,323\n",
      "Day 243: Reward (in $) = 1,562\n",
      "Day 244: Reward (in $) = -65,607\n",
      "Day 245: Reward (in $) = -121,215\n",
      "Day 246: Reward (in $) = -68,379\n",
      "Day 247: Reward (in $) = -7,723\n",
      "Day 248: Reward (in $) = -27,903\n",
      "Day 249: Reward (in $) = -34,321\n",
      "Day 250: Reward (in $) = -22,134\n",
      "Day 251: Reward (in $) = 283\n",
      "Day 252: Reward (in $) = 1,289\n",
      "Day 253: Reward (in $) = 118,794\n",
      "Day 254: Reward (in $) = -15,018\n",
      "Day 255: Reward (in $) = -72,164\n",
      "Day 256: Reward (in $) = -6,066\n",
      "Day 257: Reward (in $) = 3,737,193\n",
      "Day 258: Reward (in $) = -15,515\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 259: Reward (in $) = -53,332\n",
      "Day 260: Reward (in $) = -36,631\n",
      "Day 261: Reward (in $) = 27,432\n",
      "Day 262: Reward (in $) = 12,445\n",
      "Day 263: Reward (in $) = -33,103\n",
      "Day 264: Reward (in $) = -41,163\n",
      "Day 265: Reward (in $) = -5,949\n",
      "Day 266: Reward (in $) = -21,852\n",
      "Day 267: Reward (in $) = -10,035\n",
      "Day 268: Reward (in $) = -28,738\n",
      "Day 269: Reward (in $) = -104,879\n",
      "Day 270: Reward (in $) = -109,768\n",
      "Day 271: Reward (in $) = -13,794\n",
      "Day 272: Reward (in $) = -42,492\n",
      "Day 273: Reward (in $) = -42,903\n",
      "Day 274: Reward (in $) = 18,329\n",
      "Day 275: Reward (in $) = 30,277\n",
      "Day 276: Reward (in $) = 63,224\n",
      "Day 277: Reward (in $) = 174,519\n",
      "Day 278: Reward (in $) = 235,593\n",
      "Day 279: Reward (in $) = 71,986\n",
      "Day 280: Reward (in $) = 17,472\n",
      "Day 281: Reward (in $) = -12,897\n",
      "Day 282: Reward (in $) = -4,758\n",
      "Day 283: Reward (in $) = -3,776\n",
      "Day 284: Reward (in $) = 27,425\n",
      "Day 285: Reward (in $) = -78,222\n",
      "Day 286: Reward (in $) = -64,710\n",
      "Day 287: Reward (in $) = -53,729\n",
      "Day 288: Reward (in $) = 16,092\n",
      "Day 289: Reward (in $) = 38,692\n",
      "Day 290: Reward (in $) = 7,640\n",
      "Day 291: Reward (in $) = -61,521\n",
      "Day 292: Reward (in $) = -18,255\n",
      "Day 293: Reward (in $) = -13,111\n",
      "Day 294: Reward (in $) = 35,767\n",
      "Day 295: Reward (in $) = -12,749\n",
      "Day 296: Reward (in $) = -5,994\n",
      "Day 297: Reward (in $) = -45,247\n",
      "Day 298: Reward (in $) = 606,512\n",
      "Day 299: Reward (in $) = -8,107\n",
      "Day 300: Reward (in $) = 11,426\n",
      "Day 301: Reward (in $) = -10,848\n",
      "Day 302: Reward (in $) = -6,911\n",
      "Day 303: Reward (in $) = 9,347\n",
      "Day 304: Reward (in $) = -36,026\n",
      "Day 305: Reward (in $) = -21,480\n",
      "Day 306: Reward (in $) = -14,458\n",
      "Day 307: Reward (in $) = 190,221\n",
      "Day 308: Reward (in $) = -63,454\n",
      "Day 309: Reward (in $) = -30,250\n",
      "Day 310: Reward (in $) = -8,656\n",
      "Day 311: Reward (in $) = -39,892\n",
      "Day 312: Reward (in $) = 52,554\n",
      "Day 313: Reward (in $) = -2,451\n",
      "Day 314: Reward (in $) = -47,354\n",
      "Day 315: Reward (in $) = -60,083\n",
      "Day 316: Reward (in $) = -27,322\n",
      "Day 317: Reward (in $) = -38,676\n",
      "Day 318: Reward (in $) = 78,287\n",
      "Day 319: Reward (in $) = 3,739\n",
      "Day 320: Reward (in $) = 26,183\n",
      "Day 321: Reward (in $) = 26,413\n",
      "Day 322: Reward (in $) = 2,983\n",
      "Day 323: Reward (in $) = 8,504\n",
      "Day 324: Reward (in $) = 20,054\n",
      "Day 325: Reward (in $) = -29,224\n",
      "Day 326: Reward (in $) = -15,776\n",
      "Day 327: Reward (in $) = 25,166\n",
      "Day 328: Reward (in $) = 138,505\n",
      "Day 329: Reward (in $) = 44,421\n",
      "Day 330: Reward (in $) = 6,663\n",
      "Day 331: Reward (in $) = -9,875\n",
      "Day 332: Reward (in $) = -59,536\n",
      "Day 333: Reward (in $) = -97,025\n",
      "Day 334: Reward (in $) = -5,406\n",
      "Day 335: Reward (in $) = -22,376\n",
      "Day 336: Reward (in $) = -29,589\n",
      "Day 337: Reward (in $) = 11,500\n",
      "Day 338: Reward (in $) = -11,703\n",
      "Day 339: Reward (in $) = -20,530\n",
      "Day 340: Reward (in $) = -516\n",
      "Day 341: Reward (in $) = -59,377\n",
      "Day 342: Reward (in $) = -23,453\n",
      "Day 343: Reward (in $) = -24,699\n",
      "Day 344: Reward (in $) = 98,295\n",
      "Day 345: Reward (in $) = 110,981\n",
      "Day 346: Reward (in $) = -46,550\n",
      "Day 347: Reward (in $) = 25,530\n",
      "Day 348: Reward (in $) = -32,165\n",
      "Day 349: Reward (in $) = 74,757\n",
      "Day 350: Reward (in $) = -4,282\n",
      "Day 351: Reward (in $) = 154,414\n",
      "Day 352: Reward (in $) = 9,433\n",
      "Day 353: Reward (in $) = -5,496\n",
      "Day 354: Reward (in $) = -54,320\n",
      "Day 355: Reward (in $) = 8,000\n",
      "Day 356: Reward (in $) = -13,128\n",
      "Day 357: Reward (in $) = -115,987\n",
      "Day 358: Reward (in $) = -5,369\n",
      "Day 359: Reward (in $) = -16,653\n",
      "Day 360: Reward (in $) = -38,787\n",
      "Day 361: Reward (in $) = -105,160\n",
      "Day 362: Reward (in $) = 26,525\n",
      "Day 363: Reward (in $) = 158,929\n",
      "Day 364: Reward (in $) = 1,555\n",
      "Day 365: Reward (in $) = -33,121\n",
      "Total money earned over the year (in $) = 10,899,459\n"
     ]
    }
   ],
   "source": [
    "dailyBudget = 300000\n",
    "quantileOffer = 0.75\n",
    "\n",
    "# Keep track of your rewards in each day, a cumulative reward over the year, and a total reward over the\n",
    "# year. Also, keep track of the total reward from each option. Store them as dictionaries that are\n",
    "# indexed by day of the year.\n",
    "reward = {}\n",
    "cumulativeReward = {}\n",
    "totalReward = 0\n",
    "optionReturn = dict((option, 0) for option in optionNames)\n",
    "\n",
    "# Implement the trading strategy on each day!\n",
    "\n",
    "for day in rangeOfDays:\n",
    "\n",
    "    reward[day] = 0\n",
    "\n",
    "    # Find the options that your classifier says that should be profitable. Store the profitable option\n",
    "    # names in chosenOptions.\n",
    "    chosenOptionNumbers = np.ravel(list(np.nonzero(predictedY.loc[day].values)))\n",
    "    if np.size(chosenOptionNumbers) == 0:\n",
    "        continue\n",
    "    chosenOptions = [optionNames[i] for i in chosenOptionNumbers]\n",
    "\n",
    "    # Design the portfolio based on average price spreads. Our strategy is that as long as you have not\n",
    "    # exceeded your daily budget, pick an option from the list of 'chosenOptions' probabilistically, where\n",
    "    # the probability of choosing it is proportional to exponential(historical rewards of that option). That\n",
    "    # is, a historically profitable option is chosen more often than one that is not. Keep sampling and\n",
    "    # decreasing your budget with each bid.\n",
    "\n",
    "    # Start with an empty portfolio.\n",
    "\n",
    "    portfolio = dict((option, 0) for option in chosenOptions)\n",
    "\n",
    "    # Calculate the probabilities of choosing each option among the list 'chosenOptions'. Recall that\n",
    "    # 'chosenOptions' contains the options that your classifier indicates as being profitable.\n",
    "    priceSpreads = [1.0 * averagePriceSpread[option] for option in chosenOptions]\n",
    "    probabilityOptions = [np.exp(p) for p in priceSpreads]\n",
    "    probabilityOptions /= np.sum(probabilityOptions)\n",
    "\n",
    "    # Start with your daily budget.\n",
    "    budget = dailyBudget\n",
    "\n",
    "    # Sampling among the profitable options and bid based on them.\n",
    "    while budget > np.median([offerPrices[quantileOffer][option] for option in chosenOptions]):\n",
    "\n",
    "        optionToBuy = choice(chosenOptions, p=probabilityOptions)\n",
    "\n",
    "        if budget >= offerPrices[quantileOffer][optionToBuy]:\n",
    "            portfolio[optionToBuy] += 1\n",
    "            budget -= offerPrices[quantileOffer][optionToBuy]\n",
    "\n",
    "    # Compute the reward from the day. Go through each of the options you have decided to buy.\n",
    "    # If the DA price is lower than the bid price, then your bid is cleared. For each option you\n",
    "    # have bought, you get a reward equal to the DA - RT price.\n",
    "\n",
    "    for option in chosenOptions:\n",
    "        if testPriceDA.at[day, option] < offerPrices[quantileOffer][option]:\n",
    "            rewardOptionDay = testPriceDART.at[day, option] * portfolio[option]\n",
    "            optionReturn[option] += rewardOptionDay\n",
    "            reward[day] += rewardOptionDay\n",
    "\n",
    "    totalReward += reward[day]\n",
    "    \n",
    "    # Calculate the cumulative reward in millions of dollars.\n",
    "    cumulativeReward[day] = totalReward/1000000\n",
    "\n",
    "    print(\"Day \" + str(day) + \": Reward (in $) = \" + \"{0:,.0f}\".format(reward[day]))\n",
    "\n",
    "print(\"Total money earned over the year (in $) = \" + \"{0:,.0f}\".format(totalReward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6: Beat the algorithm! (25 points, bonus) \n",
    "\n",
    "Make changes to increase the profits further! The people with the top 20 total profits will get additional points. Exaplain what did for improvement and how much profits you made. Do not exceed the 250,000 budget!\n",
    "\n",
    "#### (comments here, add a new code cell below)\n",
    "Total money earned over the year (in $) = 8,108,141\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 3: Reward (in $) = -73,971\n",
      "Day 4: Reward (in $) = 3,243,691\n",
      "Day 5: Reward (in $) = 21,459\n",
      "Day 6: Reward (in $) = -70,558\n",
      "Day 7: Reward (in $) = -4,361\n",
      "Day 8: Reward (in $) = -42,408\n",
      "Day 9: Reward (in $) = -25,480\n",
      "Day 10: Reward (in $) = -104,479\n",
      "Day 11: Reward (in $) = -77,953\n",
      "Day 12: Reward (in $) = -78,256\n",
      "Day 13: Reward (in $) = -67,494\n",
      "Day 14: Reward (in $) = -39,942\n",
      "Day 15: Reward (in $) = -8,844\n",
      "Day 16: Reward (in $) = 35,693\n",
      "Day 17: Reward (in $) = -62,604\n",
      "Day 18: Reward (in $) = -26,495\n",
      "Day 19: Reward (in $) = 5,312\n",
      "Day 20: Reward (in $) = -6,000\n",
      "Day 21: Reward (in $) = 25,897\n",
      "Day 22: Reward (in $) = 85,329\n",
      "Day 23: Reward (in $) = 28,457\n",
      "Day 24: Reward (in $) = -77,692\n",
      "Day 25: Reward (in $) = -21,072\n",
      "Day 26: Reward (in $) = -17,079\n",
      "Day 27: Reward (in $) = -11,051\n",
      "Day 28: Reward (in $) = -20,702\n",
      "Day 29: Reward (in $) = -22,177\n",
      "Day 30: Reward (in $) = -56,101\n",
      "Day 31: Reward (in $) = -25,314\n",
      "Day 32: Reward (in $) = -7,820\n",
      "Day 33: Reward (in $) = -68,065\n",
      "Day 34: Reward (in $) = -9,414\n",
      "Day 35: Reward (in $) = 15,868\n",
      "Day 36: Reward (in $) = -34,108\n",
      "Day 37: Reward (in $) = -38,050\n",
      "Day 38: Reward (in $) = 20,367\n",
      "Day 39: Reward (in $) = 12,862\n",
      "Day 40: Reward (in $) = -32,237\n",
      "Day 41: Reward (in $) = -42,797\n",
      "Day 42: Reward (in $) = -13,039\n",
      "Day 43: Reward (in $) = -49,626\n",
      "Day 44: Reward (in $) = -30,926\n",
      "Day 45: Reward (in $) = 44,923\n",
      "Day 46: Reward (in $) = 78,944\n",
      "Day 47: Reward (in $) = 43,557\n",
      "Day 48: Reward (in $) = -51,408\n",
      "Day 49: Reward (in $) = -57,771\n",
      "Day 50: Reward (in $) = -49,796\n",
      "Day 51: Reward (in $) = 9,294\n",
      "Day 52: Reward (in $) = -17,711\n",
      "Day 53: Reward (in $) = -336\n",
      "Day 54: Reward (in $) = -6,065\n",
      "Day 55: Reward (in $) = -4,030\n",
      "Day 56: Reward (in $) = -42,299\n",
      "Day 57: Reward (in $) = -27,093\n",
      "Day 58: Reward (in $) = -18,604\n",
      "Day 59: Reward (in $) = -14,562\n",
      "Day 60: Reward (in $) = -59,565\n",
      "Day 61: Reward (in $) = -145,900\n",
      "Day 62: Reward (in $) = -3,301\n",
      "Day 63: Reward (in $) = -12,167\n",
      "Day 64: Reward (in $) = 1,644\n",
      "Day 65: Reward (in $) = 137,334\n",
      "Day 66: Reward (in $) = 116,391\n",
      "Day 67: Reward (in $) = -12,081\n",
      "Day 68: Reward (in $) = -31,747\n",
      "Day 69: Reward (in $) = -83,860\n",
      "Day 70: Reward (in $) = 71,839\n",
      "Day 71: Reward (in $) = -8,712\n",
      "Day 72: Reward (in $) = -11,271\n",
      "Day 73: Reward (in $) = -102,994\n",
      "Day 74: Reward (in $) = 692,055\n",
      "Day 75: Reward (in $) = -16,248\n",
      "Day 76: Reward (in $) = -93,449\n",
      "Day 77: Reward (in $) = -71,444\n",
      "Day 78: Reward (in $) = -24,764\n",
      "Day 79: Reward (in $) = -32,753\n",
      "Day 80: Reward (in $) = -25,068\n",
      "Day 81: Reward (in $) = -13,838\n",
      "Day 82: Reward (in $) = 12,986\n",
      "Day 83: Reward (in $) = -7,292\n",
      "Day 84: Reward (in $) = -10,828\n",
      "Day 85: Reward (in $) = 24,446\n",
      "Day 86: Reward (in $) = -33,018\n",
      "Day 87: Reward (in $) = 3,021,435\n",
      "Day 88: Reward (in $) = 925,723\n",
      "Day 89: Reward (in $) = -47,613\n",
      "Day 90: Reward (in $) = -13,735\n",
      "Day 91: Reward (in $) = -14,891\n",
      "Day 92: Reward (in $) = -26,419\n",
      "Day 93: Reward (in $) = 2,825\n",
      "Day 94: Reward (in $) = 10,930\n",
      "Day 95: Reward (in $) = -7,444\n",
      "Day 96: Reward (in $) = 69,374\n",
      "Day 97: Reward (in $) = 7,839\n",
      "Day 98: Reward (in $) = -80,339\n",
      "Day 99: Reward (in $) = -31,859\n",
      "Day 100: Reward (in $) = -21,927\n",
      "Day 101: Reward (in $) = -2,577\n",
      "Day 102: Reward (in $) = -66\n",
      "Day 103: Reward (in $) = 197,446\n",
      "Day 104: Reward (in $) = -67,451\n",
      "Day 105: Reward (in $) = -6,921\n",
      "Day 106: Reward (in $) = 15,728\n",
      "Day 107: Reward (in $) = -13,980\n",
      "Day 108: Reward (in $) = 41,172\n",
      "Day 109: Reward (in $) = -22,995\n",
      "Day 110: Reward (in $) = -40,894\n",
      "Day 111: Reward (in $) = -24,670\n",
      "Day 112: Reward (in $) = 77,146\n",
      "Day 113: Reward (in $) = -57,237\n",
      "Day 114: Reward (in $) = -57,668\n",
      "Day 115: Reward (in $) = -122,396\n",
      "Day 116: Reward (in $) = 772\n",
      "Day 117: Reward (in $) = -37,347\n",
      "Day 118: Reward (in $) = -17,316\n",
      "Day 119: Reward (in $) = -3,208\n",
      "Day 120: Reward (in $) = -4,740\n",
      "Day 121: Reward (in $) = 56,897\n",
      "Day 122: Reward (in $) = -70,264\n",
      "Day 123: Reward (in $) = -54,974\n",
      "Day 124: Reward (in $) = -37,542\n",
      "Day 125: Reward (in $) = -26,482\n",
      "Day 126: Reward (in $) = -59,077\n",
      "Day 127: Reward (in $) = -9,770\n",
      "Day 128: Reward (in $) = -48,845\n",
      "Day 129: Reward (in $) = -80,973\n",
      "Day 130: Reward (in $) = -31,424\n",
      "Day 131: Reward (in $) = 20,629\n",
      "Day 132: Reward (in $) = 113,585\n",
      "Day 133: Reward (in $) = -98,789\n",
      "Day 134: Reward (in $) = -44,172\n",
      "Day 135: Reward (in $) = -10,326\n",
      "Day 136: Reward (in $) = -51,286\n",
      "Day 137: Reward (in $) = -42,609\n",
      "Day 138: Reward (in $) = -8,675\n",
      "Day 139: Reward (in $) = -5,216\n",
      "Day 140: Reward (in $) = -37,508\n",
      "Day 141: Reward (in $) = 15,600\n",
      "Day 142: Reward (in $) = 21,668\n",
      "Day 143: Reward (in $) = 3,301\n",
      "Day 144: Reward (in $) = -53,881\n",
      "Day 145: Reward (in $) = -41,972\n",
      "Day 146: Reward (in $) = -21,454\n",
      "Day 147: Reward (in $) = -43,295\n",
      "Day 148: Reward (in $) = 314,307\n",
      "Day 149: Reward (in $) = 58,309\n",
      "Day 150: Reward (in $) = 48,673\n",
      "Day 151: Reward (in $) = 474,829\n",
      "Day 152: Reward (in $) = 326,241\n",
      "Day 153: Reward (in $) = 59,712\n",
      "Day 154: Reward (in $) = 7,413\n",
      "Day 155: Reward (in $) = -38,784\n",
      "Day 156: Reward (in $) = -40,656\n",
      "Day 157: Reward (in $) = -141,735\n",
      "Day 158: Reward (in $) = -68,524\n",
      "Day 159: Reward (in $) = -34,262\n",
      "Day 160: Reward (in $) = -130,490\n",
      "Day 161: Reward (in $) = 2,809\n",
      "Day 162: Reward (in $) = 151,398\n",
      "Day 163: Reward (in $) = -12,775\n",
      "Day 164: Reward (in $) = -39,052\n",
      "Day 165: Reward (in $) = 137,780\n",
      "Day 166: Reward (in $) = 8,791\n",
      "Day 167: Reward (in $) = 12,217\n",
      "Day 168: Reward (in $) = -29,366\n",
      "Day 169: Reward (in $) = -60,026\n",
      "Day 170: Reward (in $) = -32,132\n",
      "Day 171: Reward (in $) = -58,538\n",
      "Day 172: Reward (in $) = -28,517\n",
      "Day 173: Reward (in $) = -1,191\n",
      "Day 174: Reward (in $) = -54,853\n",
      "Day 175: Reward (in $) = 25,302\n",
      "Day 176: Reward (in $) = -16,685\n",
      "Day 177: Reward (in $) = -61,755\n",
      "Day 178: Reward (in $) = -17,463\n",
      "Day 179: Reward (in $) = -29,398\n",
      "Day 180: Reward (in $) = -36,038\n",
      "Day 181: Reward (in $) = -19,985\n",
      "Day 182: Reward (in $) = -65,293\n",
      "Day 183: Reward (in $) = -74,550\n",
      "Day 184: Reward (in $) = -84,584\n",
      "Day 185: Reward (in $) = -40,204\n",
      "Day 186: Reward (in $) = 69,094\n",
      "Day 187: Reward (in $) = 9,106\n",
      "Day 188: Reward (in $) = 259\n",
      "Day 189: Reward (in $) = -6,182\n",
      "Day 190: Reward (in $) = -99,873\n",
      "Day 191: Reward (in $) = -21,289\n",
      "Day 192: Reward (in $) = -26,064\n",
      "Day 193: Reward (in $) = -57,119\n",
      "Day 194: Reward (in $) = -4,346\n",
      "Day 195: Reward (in $) = 759\n",
      "Day 196: Reward (in $) = 5,795\n",
      "Day 197: Reward (in $) = 8,104\n",
      "Day 198: Reward (in $) = 95,476\n",
      "Day 199: Reward (in $) = -2,983\n",
      "Day 200: Reward (in $) = -42,428\n",
      "Day 201: Reward (in $) = -11,496\n",
      "Day 202: Reward (in $) = 1,107\n",
      "Day 203: Reward (in $) = 4,517\n",
      "Day 204: Reward (in $) = -12,930\n",
      "Day 205: Reward (in $) = -71,095\n",
      "Day 206: Reward (in $) = -3,452\n",
      "Day 207: Reward (in $) = -4,744\n",
      "Day 208: Reward (in $) = 1,834\n",
      "Day 209: Reward (in $) = 10,671\n",
      "Day 210: Reward (in $) = 109,659\n",
      "Day 211: Reward (in $) = 42,929\n",
      "Day 212: Reward (in $) = 75,688\n",
      "Day 213: Reward (in $) = -4,033\n",
      "Day 214: Reward (in $) = -53,016\n",
      "Day 215: Reward (in $) = -17,811\n",
      "Day 216: Reward (in $) = 4,795\n",
      "Day 217: Reward (in $) = -32,718\n",
      "Day 218: Reward (in $) = 24,245\n",
      "Day 219: Reward (in $) = -27,135\n",
      "Day 220: Reward (in $) = -34,059\n",
      "Day 221: Reward (in $) = -16,033\n",
      "Day 222: Reward (in $) = 18,524\n",
      "Day 223: Reward (in $) = 3,583\n",
      "Day 224: Reward (in $) = 3,087\n",
      "Day 225: Reward (in $) = 32,211\n",
      "Day 226: Reward (in $) = -44,507\n",
      "Day 227: Reward (in $) = 54,890\n",
      "Day 228: Reward (in $) = 1,797\n",
      "Day 229: Reward (in $) = -6,953\n",
      "Day 230: Reward (in $) = -1,964\n",
      "Day 231: Reward (in $) = 6,156\n",
      "Day 232: Reward (in $) = -48,445\n",
      "Day 233: Reward (in $) = -44,632\n",
      "Day 234: Reward (in $) = -93,933\n",
      "Day 235: Reward (in $) = -122,639\n",
      "Day 236: Reward (in $) = -19,530\n",
      "Day 237: Reward (in $) = 45,629\n",
      "Day 238: Reward (in $) = -11,927\n",
      "Day 239: Reward (in $) = -52,005\n",
      "Day 240: Reward (in $) = -3,974\n",
      "Day 241: Reward (in $) = -1,444\n",
      "Day 242: Reward (in $) = -76,318\n",
      "Day 243: Reward (in $) = -4,181\n",
      "Day 244: Reward (in $) = -49,283\n",
      "Day 245: Reward (in $) = -93,202\n",
      "Day 246: Reward (in $) = -50,792\n",
      "Day 247: Reward (in $) = -5,581\n",
      "Day 248: Reward (in $) = -20,507\n",
      "Day 249: Reward (in $) = -19,515\n",
      "Day 250: Reward (in $) = -19,120\n",
      "Day 251: Reward (in $) = 121\n",
      "Day 252: Reward (in $) = 5,372\n",
      "Day 253: Reward (in $) = 91,903\n",
      "Day 254: Reward (in $) = -11,628\n",
      "Day 255: Reward (in $) = -55,071\n",
      "Day 256: Reward (in $) = -4,933\n",
      "Day 257: Reward (in $) = 2,775,938\n",
      "Day 258: Reward (in $) = -12,616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day 259: Reward (in $) = -39,570\n",
      "Day 260: Reward (in $) = -26,795\n",
      "Day 261: Reward (in $) = 19,451\n",
      "Day 262: Reward (in $) = 8,738\n",
      "Day 263: Reward (in $) = -26,111\n",
      "Day 264: Reward (in $) = -36,032\n",
      "Day 265: Reward (in $) = -6,278\n",
      "Day 266: Reward (in $) = -67,997\n",
      "Day 267: Reward (in $) = -8,206\n",
      "Day 268: Reward (in $) = -23,134\n",
      "Day 269: Reward (in $) = -83,074\n",
      "Day 270: Reward (in $) = -86,865\n",
      "Day 271: Reward (in $) = -9,653\n",
      "Day 272: Reward (in $) = -32,584\n",
      "Day 273: Reward (in $) = -31,743\n",
      "Day 274: Reward (in $) = 12,809\n",
      "Day 275: Reward (in $) = 23,874\n",
      "Day 276: Reward (in $) = 48,594\n",
      "Day 277: Reward (in $) = 136,224\n",
      "Day 278: Reward (in $) = 181,064\n",
      "Day 279: Reward (in $) = 52,827\n",
      "Day 280: Reward (in $) = 12,347\n",
      "Day 281: Reward (in $) = -9,543\n",
      "Day 282: Reward (in $) = -2,700\n",
      "Day 283: Reward (in $) = -3,066\n",
      "Day 284: Reward (in $) = 21,191\n",
      "Day 285: Reward (in $) = -61,005\n",
      "Day 286: Reward (in $) = -50,121\n",
      "Day 287: Reward (in $) = -42,061\n",
      "Day 288: Reward (in $) = 12,854\n",
      "Day 289: Reward (in $) = 29,834\n",
      "Day 290: Reward (in $) = 6,143\n",
      "Day 291: Reward (in $) = -45,484\n",
      "Day 292: Reward (in $) = -17,053\n",
      "Day 293: Reward (in $) = -10,401\n",
      "Day 294: Reward (in $) = 27,684\n",
      "Day 295: Reward (in $) = -11,394\n",
      "Day 296: Reward (in $) = -1,456\n",
      "Day 297: Reward (in $) = -36,077\n",
      "Day 298: Reward (in $) = 472,202\n",
      "Day 299: Reward (in $) = -7,102\n",
      "Day 300: Reward (in $) = 8,369\n",
      "Day 301: Reward (in $) = -9,014\n",
      "Day 302: Reward (in $) = -5,222\n",
      "Day 303: Reward (in $) = 3,509\n",
      "Day 304: Reward (in $) = -53,609\n",
      "Day 305: Reward (in $) = -16,603\n",
      "Day 306: Reward (in $) = -9,971\n",
      "Day 307: Reward (in $) = 145,945\n",
      "Day 308: Reward (in $) = -48,813\n",
      "Day 309: Reward (in $) = -23,166\n",
      "Day 310: Reward (in $) = -6,920\n",
      "Day 311: Reward (in $) = -31,236\n",
      "Day 312: Reward (in $) = 41,778\n",
      "Day 313: Reward (in $) = -1,997\n",
      "Day 314: Reward (in $) = -36,246\n",
      "Day 315: Reward (in $) = -47,172\n",
      "Day 316: Reward (in $) = -21,152\n",
      "Day 317: Reward (in $) = -31,174\n",
      "Day 318: Reward (in $) = 61,420\n",
      "Day 319: Reward (in $) = 2,961\n",
      "Day 320: Reward (in $) = 20,078\n",
      "Day 321: Reward (in $) = 20,716\n",
      "Day 322: Reward (in $) = 2,194\n",
      "Day 323: Reward (in $) = 6,699\n",
      "Day 324: Reward (in $) = 16,012\n",
      "Day 325: Reward (in $) = -22,624\n",
      "Day 326: Reward (in $) = -12,828\n",
      "Day 327: Reward (in $) = 20,476\n",
      "Day 328: Reward (in $) = 108,896\n",
      "Day 329: Reward (in $) = 35,012\n",
      "Day 330: Reward (in $) = 5,141\n",
      "Day 331: Reward (in $) = -4,074\n",
      "Day 332: Reward (in $) = -46,986\n",
      "Day 333: Reward (in $) = -75,000\n",
      "Day 334: Reward (in $) = -4,533\n",
      "Day 335: Reward (in $) = -17,835\n",
      "Day 336: Reward (in $) = -23,488\n",
      "Day 337: Reward (in $) = 9,536\n",
      "Day 338: Reward (in $) = -8,995\n",
      "Day 339: Reward (in $) = -30,796\n",
      "Day 340: Reward (in $) = -37,758\n",
      "Day 341: Reward (in $) = -46,161\n",
      "Day 342: Reward (in $) = -2,055\n",
      "Day 343: Reward (in $) = -19,011\n",
      "Day 344: Reward (in $) = 75,879\n",
      "Day 345: Reward (in $) = 88,041\n",
      "Day 346: Reward (in $) = -37,096\n",
      "Day 347: Reward (in $) = 19,722\n",
      "Day 348: Reward (in $) = -27,890\n",
      "Day 349: Reward (in $) = 58,773\n",
      "Day 350: Reward (in $) = -3,455\n",
      "Day 351: Reward (in $) = 123,918\n",
      "Day 352: Reward (in $) = 7,654\n",
      "Day 353: Reward (in $) = -9,939\n",
      "Day 354: Reward (in $) = -43,966\n",
      "Day 355: Reward (in $) = 5,370\n",
      "Day 356: Reward (in $) = -12,461\n",
      "Day 357: Reward (in $) = -93,518\n",
      "Day 358: Reward (in $) = -4,442\n",
      "Day 359: Reward (in $) = -11,539\n",
      "Day 360: Reward (in $) = -31,533\n",
      "Day 361: Reward (in $) = -80,053\n",
      "Day 362: Reward (in $) = 20,067\n",
      "Day 363: Reward (in $) = 123,349\n",
      "Day 364: Reward (in $) = 1,289\n",
      "Day 365: Reward (in $) = -27,428\n",
      "Total money earned over the year (in $) = 8,108,141\n"
     ]
    }
   ],
   "source": [
    "dailyBudget = 250000\n",
    "quantileOffer = 0.75\n",
    "\n",
    "# Keep track of your rewards in each day, a cumulative reward over the year, and a total reward over the\n",
    "# year. Also, keep track of the total reward from each option. Store them as dictionaries that are\n",
    "# indexed by day of the year.\n",
    "reward = {}\n",
    "cumulativeReward = {}\n",
    "totalReward = 0\n",
    "optionReturn = dict((option, 0) for option in optionNames)\n",
    "\n",
    "# Implement the trading strategy on each day!\n",
    "\n",
    "for day in rangeOfDays:\n",
    "\n",
    "    reward[day] = 0\n",
    "\n",
    "    # Find the options that your classifier says that should be profitable. Store the profitable option\n",
    "    # names in chosenOptions.\n",
    "    chosenOptionNumbers = np.ravel(list(np.nonzero(predictedY.loc[day].values)))\n",
    "    if np.size(chosenOptionNumbers) == 0:\n",
    "        continue\n",
    "    chosenOptions = [optionNames[i] for i in chosenOptionNumbers]\n",
    "\n",
    "    # Design the portfolio based on average price spreads. Our strategy is that as long as you have not\n",
    "    # exceeded your daily budget, pick an option from the list of 'chosenOptions' probabilistically, where\n",
    "    # the probability of choosing it is proportional to exponential(historical rewards of that option). That\n",
    "    # is, a historically profitable option is chosen more often than one that is not. Keep sampling and\n",
    "    # decreasing your budget with each bid.\n",
    "\n",
    "    # Start with an empty portfolio.\n",
    "\n",
    "    portfolio = dict((option, 0) for option in chosenOptions)\n",
    "\n",
    "    # Calculate the probabilities of choosing each option among the list 'chosenOptions'. Recall that\n",
    "    # 'chosenOptions' contains the options that your classifier indicates as being profitable.\n",
    "    priceSpreads = [1.0 * averagePriceSpread[option] for option in chosenOptions]\n",
    "    probabilityOptions = [np.exp(p) for p in priceSpreads]\n",
    "    probabilityOptions /= np.sum(probabilityOptions)\n",
    "\n",
    "    # Start with your daily budget.\n",
    "    budget = dailyBudget\n",
    "\n",
    "    # Sampling among the profitable options and bid based on them.\n",
    "    while budget > np.median([offerPrices[quantileOffer][option] for option in chosenOptions]):\n",
    "\n",
    "        optionToBuy = choice(chosenOptions, p=probabilityOptions)\n",
    "\n",
    "        if budget >= offerPrices[quantileOffer][optionToBuy]:\n",
    "            portfolio[optionToBuy] += 1\n",
    "            budget -= offerPrices[quantileOffer][optionToBuy]\n",
    "\n",
    "    # Compute the reward from the day. Go through each of the options you have decided to buy.\n",
    "    # If the DA price is lower than the bid price, then your bid is cleared. For each option you\n",
    "    # have bought, you get a reward equal to the DA - RT price.\n",
    "\n",
    "    for option in chosenOptions:\n",
    "        if testPriceDA.at[day, option] < offerPrices[quantileOffer][option]:\n",
    "            rewardOptionDay = testPriceDART.at[day, option] * portfolio[option]\n",
    "            optionReturn[option] += rewardOptionDay\n",
    "            reward[day] += rewardOptionDay\n",
    "\n",
    "    totalReward += reward[day]\n",
    "    \n",
    "    # Edit to change the Quantile based on whether Reward is Positive or not \n",
    "    if reward[day] <= -100000 and quantileOffer > 0.70 : \n",
    "        quantileOffer -= 0.05\n",
    "        quantileOffer = np.around(quantileOffer, 2)\n",
    "    \n",
    "    if reward[day] >= 10000 and quantileOffer < 0.80 :\n",
    "        quantileOffer += 0.1\n",
    "        quantileOffer = np.around(quantileOffer, 2)\n",
    "    \n",
    "    if reward[day] <= -100000 and dailyBudget > 100000 : \n",
    "        dailyBudget -= 10000 \n",
    "        \n",
    "    if reward[day] >= 100000 and dailyBudget < 250000 : \n",
    "        dailyBudget += 10000     \n",
    "\n",
    "    \n",
    "    # Calculate the cumulative reward in millions of dollars.\n",
    "    cumulativeReward[day] = totalReward/1000000\n",
    "\n",
    "    print(\"Day \" + str(day) + \": Reward (in $) = \" + \"{0:,.0f}\".format(reward[day]))\n",
    "\n",
    "print(\"Total money earned over the year (in $) = \" + \"{0:,.0f}\".format(totalReward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7: Implement a different trading strategy (25 points, bonus) \n",
    "\n",
    "Implement a different trading strategy, explain it and report its outcomes. If the rewards are comparable to the one discussed in class, and you can clearly explain the logic behind you strategy, you get additional 25 points. If your strategy leads to having profits among the top 20, then you automatically completed Task 6 too! \n",
    "\n",
    "#### (comments here, add a new code cell below)\n",
    "\n",
    "<br>There are 2 trading strategies that I thought could be done here: Instead of taking the average for the entire year, the average can be taken for several months or changing the quantile variable based on the amount of profit earned. The reason why taking an average for several months would yield better results would be due to market not being consistent for the entire year. There will be some parts of the years where there will be a higher power demand compared to another part of a year and thus by taking only an average of within a few weeks of the current day could lead us to a better estimation of the price for that day. </br>\n",
    "\n",
    "<br> Secondly, the other strategy that I thought of could be to change the budget/quantile based on the profit earned on that day. If the profit ended up being lower than a given threshold, we could then penalize this decision by loweirng the quantile and budget amount. On the other hand if we earn more than a given threshold, we could then increase our quantile and budget. In the implementation above, the case in which we earn a high profit will be rewarded more than a penalization due to a large amount of small losses over the days. </br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
